<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="images/favicon.png" />
<title>Stan Programming | Bret Alexander Beheim</title>
<meta name="title" content="Stan Programming" />
<meta name="description" content="Programming in Stan recommended lit Applied Gaussian Processes in Stan Andre Zapic CmdStan User’s Guide Version 2.28 Stan Development Team (see also Stan Functions Reference Version 2.28 Stan Development Team) Stan Reference Manual Version 2.28 Stan Development Team Stan Modeling Language User’s Guide and Reference Manual Stan Development Team Stan Version 2.17.0 Stan User’s Guide Version 2.28 Stan Development Team https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919 [disclosure: Richard was on my PhD committee and I read several early drafts of this book] brms: An R Package for Bayesian Multilevel Models Using Stan 0 - setting up Before a Stan program can execute, it has to be compiled." />
<meta name="keywords" content="" />


<meta property="og:title" content="Stan Programming" />
<meta property="og:description" content="Programming in Stan recommended lit Applied Gaussian Processes in Stan Andre Zapic CmdStan User’s Guide Version 2.28 Stan Development Team (see also Stan Functions Reference Version 2.28 Stan Development Team) Stan Reference Manual Version 2.28 Stan Development Team Stan Modeling Language User’s Guide and Reference Manual Stan Development Team Stan Version 2.17.0 Stan User’s Guide Version 2.28 Stan Development Team https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919 [disclosure: Richard was on my PhD committee and I read several early drafts of this book] brms: An R Package for Bayesian Multilevel Models Using Stan 0 - setting up Before a Stan program can execute, it has to be compiled." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/notebooks/stan/" /><meta property="og:image" content="images/bret-head.jpg"/><meta property="article:section" content="notebooks" />

<meta property="og:site_name" content="Bret Alexander Beheim" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="images/bret-head.jpg"/>

<meta name="twitter:title" content="Stan Programming"/>
<meta name="twitter:description" content="Programming in Stan recommended lit Applied Gaussian Processes in Stan Andre Zapic CmdStan User’s Guide Version 2.28 Stan Development Team (see also Stan Functions Reference Version 2.28 Stan Development Team) Stan Reference Manual Version 2.28 Stan Development Team Stan Modeling Language User’s Guide and Reference Manual Stan Development Team Stan Version 2.17.0 Stan User’s Guide Version 2.28 Stan Development Team https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919 [disclosure: Richard was on my PhD committee and I read several early drafts of this book] brms: An R Package for Bayesian Multilevel Models Using Stan 0 - setting up Before a Stan program can execute, it has to be compiled."/>
<meta name="twitter:site" content="@babeheim"/>



<meta itemprop="name" content="Stan Programming">
<meta itemprop="description" content="Programming in Stan recommended lit Applied Gaussian Processes in Stan Andre Zapic CmdStan User’s Guide Version 2.28 Stan Development Team (see also Stan Functions Reference Version 2.28 Stan Development Team) Stan Reference Manual Version 2.28 Stan Development Team Stan Modeling Language User’s Guide and Reference Manual Stan Development Team Stan Version 2.17.0 Stan User’s Guide Version 2.28 Stan Development Team https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919 [disclosure: Richard was on my PhD committee and I read several early drafts of this book] brms: An R Package for Bayesian Multilevel Models Using Stan 0 - setting up Before a Stan program can execute, it has to be compiled.">

<meta itemprop="wordCount" content="16401"><meta itemprop="image" content="images/bret-head.jpg"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>

  html, body, h1, h2, h3, h4, h5, h6, p {
    font-family: "Times New Roman", Times, serif;
     
  }

  p, .sidebar, ol, ul, dl {
    line-height: 1.6;
     
    font-family: "Times New Roman", Times, serif;
    font-size: 1.2em;
    margin-bottom: 1.0em;
  }
   

  body {
    font-family: "Times New Roman", Times, serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #ffffff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 90%;
  }

  code {
    padding: 2px 5px;
    background-color: #f5f3ff;
  }
   

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }

</style>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<style>
  .twitter-tweet-rendered {
    display:block;
    margin-left:auto;
    margin-right:auto;
  }
  .inline-svg {
    display: inline-block;
    height: 1.0rem;
    width: 1.0rem;
    top: 0.15rem;
    position: relative;
    fill: #636363;
  }
  .inline-svg:hover {
    fill: #bdbdbd;
    height: 1.0rem;
    width: 1.0rem;
  }
  .inline-svg#pdf {
    fill: #de2d26;
  }
  .inline-svg#pdf:hover {
    fill: #fc9272;
  }
  .inline-svg#data {
    fill: #1c9099;
  }
  .inline-svg#data:hover {
    fill: #a6bddb;
  }
</style>
</head>

<body>
  <header><a href="/" class="title">
  <h2>Bret Alexander Beheim</h2>
</a>
<nav><a href="/">About</a>

<a href="/research/">Research</a>

<a href="/publications/">Publications</a>

<a href="/people/">People</a>

<a href="/software/">Software</a>

<a href="/teaching/">Teaching</a>

<a href="/notes/">Notebooks</a>


<a href="/blog">Blog</a>

</nav>
</header>
  <main>

<content>
  <h1 id="programming-in-stan">Programming in Stan</h1>
<h2 id="recommended-lit">recommended lit</h2>
<ul>
<li>Applied Gaussian Processes in Stan
Andre Zapic</li>
<li>CmdStan User’s Guide
Version 2.28
Stan Development Team (see also Stan Functions Reference
Version 2.28
Stan Development Team)</li>
<li>Stan Reference Manual
Version 2.28
Stan Development Team</li>
<li>Stan Modeling Language
User’s Guide and Reference Manual
Stan Development Team
Stan Version 2.17.0</li>
<li>Stan User’s Guide
Version 2.28
Stan Development Team</li>
<li><a href="https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919">https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919</a> [disclosure: Richard was on my PhD committee and I read several early drafts of this book]</li>
<li>brms: An R Package for Bayesian Multilevel Models Using Stan</li>
</ul>
<h1 id="0---setting-up">0 - setting up</h1>
<p>Before a Stan program can execute, it has to be <em>compiled</em>. There are a number of ways to go from the source code to the program itself.</p>
<h2 id="compiling--running-stan-code-on-the-command-line">compiling &amp; running Stan code on the command line</h2>
<p>To use the command line interface, you have to have a copy of the github repo installed locally:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/stan-dev/cmdstan.git --recursive
</span></span></code></pre></div><p>Then, for a stan source file called <code>mymodel.stan</code>, goto the cmdstan directory and type</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>make /path/to/model/mymodel
</span></span></code></pre></div><p>It interprets the stan code into C++ (called <code>mymodel.hpp</code>) and compiles a binary executable, which is just called <code>mymodel</code>.</p>
<p>To pass data into this program and create samples from it, the terminal command is</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mymodel sample data file=mydata.json
</span></span></code></pre></div><p>We can customize like so:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>mymodel sample num_samples=5000 data file=mydata.json diagnostic_file=diagnostics.csv
</span></span></code></pre></div><p>By default, it creates a rectangular <code>output.csv</code> in the current working directory with all the samples! This can be modified as <code>output file=myoutput.csv</code>. The cmdstan User&rsquo;s Guide gives the following example as a very sophisticade customization:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>/my_model random seed=12345 id=<span style="color:#a31515">${</span>i<span style="color:#a31515">}</span> <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  data file=my_model.data.json <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  output file=output_<span style="color:#a31515">${</span>i<span style="color:#a31515">}</span>.csv refresh=10 <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  sample num_warmup=2000 <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  init=my_param_inits.json <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  algorithm=hmc engine=nuts max_depth=15 <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  metric=dense_e metric_file=my_metric.json <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  stepsize=0.6555 <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  adapt delta=0.95 init_buffer=100 <span style="color:#a31515">\
</span></span></span><span style="display:flex;"><span><span style="color:#a31515"></span>  window=50 term_buffer=100
</span></span></code></pre></div><h2 id="compiling--running-stan-code-with-cmdstanr">compiling &amp; running Stan code with cmdstanr</h2>
<p>We can use the <code>cmdstan_model</code> function, which compiles the file in the same location as the source code and saves an object to the environment that is linked to the binary file.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>model &lt;- cmdstan_model(path_to_stan_code)
</span></span></code></pre></div><p>If we want to include the Stan code in an R script, we have to pass it to a temp file first before writing using the <code>write_stan_file</code> function:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_code &lt;- write_stan_file(<span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower=1&gt; J;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector&lt;lower=0&gt;[J] sigma;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[J] y;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real&lt;lower=0&gt; tau;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[J] theta_raw;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">transformed parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[J] theta = mu + tau * theta_raw;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  target += normal_lpdf(tau | 0, 10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  target += normal_lpdf(mu | 0, 10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  target += normal_lpdf(theta_raw | 0, 1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  target += normal_lpdf(y | theta, sigma);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>)
</span></span><span style="display:flex;"><span>print(stan_code) <span style="color:#008000"># returns temp path</span>
</span></span><span style="display:flex;"><span>model &lt;- cmdstan_model(stan_code)
</span></span></code></pre></div><p>We can see the code itself using the <code>$print()</code> method, and the path to the executable by <code>$exe_file()</code>. I believe we don&rsquo;t need to ever compile them again as long as we are using this computer. A similar process should be carried out on OSX, windows, etc. We can load them back into memory simply by running the above script a second time (or placing it into <code>project_support.r</code>!)</p>
<p>Also, you can interact with the binary file <em>without</em> R, once it is compiled.</p>
<p>With the sampler compiled, samples can be run using the <code>$sample()</code> method, which takes a data argument either as a named list in R, or as a path to a JSON data file.</p>
<p>The <code>$sample()</code> method outputs a <code>CmdStanMCMC</code> object to store the fits, and is the target of our subsequent analyses. If this thing is called <code>fit</code>, then we should try:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit$summary() <span style="color:#008000"># calls posterior::summarize_draws()</span>
</span></span><span style="display:flex;"><span>fit$summary(<span style="color:#a31515">&#34;theta&#34;</span>, <span style="color:#a31515">&#34;mean&#34;</span>, <span style="color:#a31515">&#34;sd&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit$draws() <span style="color:#008000"># default is 3-D array, iterations x chains x variables with no other structure</span>
</span></span><span style="display:flex;"><span>fit$draws(format = <span style="color:#a31515">&#34;df&#34;</span>) <span style="color:#008000"># makes a dataframe, with that weird nonsense about index columns</span>
</span></span><span style="display:flex;"><span>posterior::as_draws_df(fit$draws()) <span style="color:#008000"># equivalent command</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>bayesplot::mcmc_hist(fit$draws(<span style="color:#a31515">&#34;theta&#34;</span>)) <span style="color:#008000"># prints a histogram</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit$sampler_diagnostics()  <span style="color:#008000"># another draws_array object</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit$cmdstan_diagnose()  <span style="color:#008000"># key command</span>
</span></span><span style="display:flex;"><span>fit$cmdstan_summary() <span style="color:#008000"># key command</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># how to extract samples directly</span>
</span></span><span style="display:flex;"><span>post &lt;- posterior::as_draws_rvars(fit$draws())
</span></span><span style="display:flex;"><span>posterior::draws_of(post$mu)
</span></span><span style="display:flex;"><span>posterior::draws_of(post$a)
</span></span></code></pre></div><h2 id="how-to-interact-with-saved-models">how to interact with saved models</h2>
<p>When a cmdstan model fits in R, it stores the csvs as well as an object in R that saves the paths to the files. These CSVs are stashed in a temp folder somewhere, full paths can be viewed with <code>fit$output_files()</code>. However, they can also be manually specified as using the <code>output_dir</code> argument in <code>$sample()</code>. Unfortunately these are absolute paths, so fitting a mdoel on one machine and moving the files to another machine means it won&rsquo;t work if we just try to <code>save</code> the CmdStanFit R object.</p>
<p>There are two solutions. First, we could re-constitute the R object from the CSV files as so:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- read_cmdstan_csv(fit$output_files())
</span></span></code></pre></div><p>Another solution is the <code>read_stan_csv</code> function in <code>rstan</code>, which stores a stanfit object:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- rstan::read_stan_csv(fit$output_files()) <span style="color:#008000"># # this is a stanfit, not a cmdstanfit!</span>
</span></span></code></pre></div><p>of course, in the old days, I&rsquo;d use <code>rethinking::extract.samples(fit)</code> to get at the samples themselves from the rstan fit.</p>
<p>We can also directly save fitted model objects, including samples, by</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit$save_object(file = <span style="color:#a31515">&#34;fit.RDS&#34;</span>)
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span><span style="color:#008000"># load it back with</span>
</span></span><span style="display:flex;"><span>fit &lt;- readRDS(<span style="color:#a31515">&#34;fit.RDS&#34;</span>)
</span></span></code></pre></div><p>This will <em>save the samples</em> somehow too, it seems, so is very transportable.</p>
<h2 id="compiling--running-stan-code-with-rstan">compiling &amp; running Stan code with rstan</h2>
<p>rstan::stan()</p>
<p>We can compile Stan code using <code>stan_model</code>.</p>
<p>Unlike cmdstan, the RStan interface is calling C++ code within R, using Rcpp and inline. This has more memory requirements and can be tricky to use. It has the advantage of being able to <em>pass model code strings into the model compiling</em> directly, rather than first writing the model to file using <code>write_stan_file</code>.</p>
<p>So should we use Rstan at all anymore? Cmdstan itself seems <a href="https://mc-stan.org/cmdstanr/articles/cmdstanr.htm">the most up-to-date and stable version</a> of stan:</p>
<blockquote>
<p>Both forms of interfacing with Stan have advantages and disadvantages. An in-memory interface like RStan is able to offer more advanced features than CmdStanR (for example the rstan::log_prob() and rstan::grad_log_prob() methods) but keeping up with Stan releases is more complicated for RStan, often requiring non-trivial changes to the rstan package and requiring new CRAN releases of rstan and StanHeaders. On the other hand, with CmdStanR, the latest features in Stan will be available from R immediately after updating CmdStan, without an update to the cmdstanr package. We also anticipate that running Stan via external processes will have the advantage of playing nicer with R (and RStudio) and result in fewer unexpected crashes than when using RStan</p>
</blockquote>
<h1 id="1-stan-language-basics">1. Stan language basics</h1>
<h2 id="the-structure-of-a-stan-program">the structure of a stan program</h2>
<p>A typical Stan program will have three named &ldquo;block&rdquo;:</p>
<pre tabindex="0"><code>data { }
parameters { }
model { }
</code></pre><p>The model block is extremely flexible and allows many operations, but is meant to do two things:</p>
<ol>
<li>defines priors on all parameters and transformed parameters</li>
<li>uses estimated parameters and data to calculate the log probability density for the outcome variable, using the Hamiltonian MCMC algorithm.</li>
</ol>
<p>As the manual says, &ldquo;the basis of Stan&rsquo;s execution is the evaluation of a log probability function&rdquo;, either represented by a sampling statement or an incremental log density pointing at the outcome variable. Interestingly, <em>it will still run</em> without a log-probability statement; it just returns the prior in that case and performs no sampling.</p>
<p>Inside the model block, right-side statements can do multiplication and addition.</p>
<p>Optionally, you might see a <code>transformed data</code>, <code>transformed parameters</code>, or <code>generated quantities</code> blocks, which allow more complex models or elaborated output.</p>
<h2 id="stan-types">stan types</h2>
<p>(second 4.10 of the stan refernce manual)</p>
<p>Each variable has a <em>declaration type</em> and an <em>array dimensionalty</em>. The dimensionality matters a lot - the functions <code>real mean(real[]);</code> and <code>real mean(vector);</code> are different functions.</p>
<p>The primitive implementation types are <code>int</code>, <code>real</code>, <code>vector</code>, <code>row_vector</code>, <code>matrix</code>. Several types descend from the <code>matrix</code> type: <code>cov_matrix</code>, <code>corr_matrix</code>, <code>cholesky_factor_cov</code>, <code>cholesky_factor_corr</code>. Several types descend from the <code>vector</code> type: <code>simplex</code>, <code>unit_vector</code>, <code>ordered</code>, and <code>positive_ordered</code>.</p>
<p>what is a &lsquo;multiple index&rsquo;?? ah, the index is itself an integer array</p>
<p><code>a[11]</code> returns the value of <code>a</code> at index 11
if <code>ii</code> is an integer array (<code>int[]</code>) of size <code>K</code>, <code>a[ii]</code> returns <code>a[ii[1]]</code>, <code>a[ii[2]]</code>, &hellip;, <code>a[ii[K]]</code></p>
<p>both <code>a[]</code> and <code>a[:]</code> are the &lsquo;all&rsquo; index types, and return all elements of <code>a</code></p>
<p>&ldquo;integer array dimensionality&rdquo; - int[] has array dimensionality of 1, <code>int</code> has an array dimesnionality of 0, int[,,] has an array dimensioality of 3.</p>
<p>array dimensions come before matrix/vector dimensions, so if we declare <code>a</code> as follows:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">matrix[M,N] a[I,J,K]
</code></pre><p>we address a like so: <code>a[i, j, k, m, n]</code>, with the &lsquo;array&rsquo; indices i, j and k first, followed by the matrix indicies m and n. Read literally, there are <code>i</code> x <code>j</code> x <code>k</code> matricies inside <code>a</code>, each of which has dimensions <code>m</code> x <code>n</code>.</p>
<h2 id="indexing">indexing</h2>
<p>(section 4.7 of stan reference manual)</p>
<p>arrays, matrices, vectors, row vectors are all accessed using the same notation</p>
<p>if <code>x</code> is type <code>real[]</code>, a one-dimensional array, we can access the first element by <code>x[1]</code>
if <code>x</code> is type <code>real[,]</code>, a two-dimesional array, each element is a <code>real</code>. <code>x[2]</code> is then of type <code>real[,]</code>, and <code>x[2,3]</code> is of type <code>real[]</code>. Thus, <code>x[2,3]</code> and <code>x[2][3]</code> have the same meaning.</p>
<p>if <code>x</code> is type <code>matrix</code>, then <code>x[1]</code> denotes the first <em>row</em> of <code>x</code> and is of type <code>row_vector</code>.</p>
<h2 id="declaring-dimensions">declaring dimensions</h2>
<h2 id="integer-and-real-expressions">integer and real expressions</h2>
<p>All the arithmetic operations we expect work on integers and reals: +, -, *, /, as well as negation (also using -) and exponentiation (using ^). For integers there is also modulus (%).</p>
<h2 id="row-vector-vector-and-matrix-expressions">row vector, vector, and matrix expressions</h2>
<p>negation, addition, subtraction and multiplication work for matrices, vectors, and row vectors, but new operations exist too: transpose.</p>
<p>note that multiplication in matrix land is <em>matrix</em> multiplication, e.g.</p>
<p>(1xm)&rsquo; * mxm * mx1 = 1x1</p>
<p>row_vector[1,m] * matrix[m,m] -&gt; row_vector[1,m]
row_vector[1,m] * vector[m,1] -&gt; real
row_vector[m,1] * vector[1,n] -&gt; matrix[m,n]</p>
<p>element-wise matrix divison and multiplication also exist as operations, but are basically equivalent in speed and efficiency as loops</p>
<p>A row vector expression is based on square brackets:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">row_vector[5] x = [1, 2, 3, 4, 5];
</code></pre><p>A &lsquo;vector&rsquo; in stan is actually meaning a <em>column</em> vector, and can be created by transposing a row vector:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">vector[5] xp = x&#39;;
vector[5] y = [1, 2, 3, 4, 5]&#39;;
</code></pre><p>A matrix expression is defined in Stan as a sequence of comma-separated row vector expressions surrounded by square brackets. The row vector expressions are the &lsquo;rows&rsquo; of the matrix, so take the second dimension:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">matrix[2,4] [[1, 2, 3, 4], [5, 6, 7, 8]]
</code></pre><h2 id="arrays">arrays</h2>
<h2 id="multiple-indexing">multiple indexing</h2>
<p>a multiple index works like this:
a[idxs, &hellip;][i, &hellip;]
becomes
a[idxs[i],&hellip;][&hellip;]</p>
<p>If the Stan declaration is</p>
<pre tabindex="0"><code>vector&lt;lower = 0&gt;[K] beta[J]
</code></pre><p>this declares <code>beta</code> to be an array of size <code>J</code>, each element of which is a vector of length <code>K</code>, with the constraint that no value can go below 0. Likewise,</p>
<pre tabindex="0"><code>  matrix[7, 2] mu[15, 12];
</code></pre><p>declares a 15 x 12 array made up of 7 x 2 matricies.</p>
<p>What to do if you have two alternative ways to declare the same thing, e.g.</p>
<pre tabindex="0"><code>  vector[K] beta[K]
  matrix[K, K] beta
</code></pre><p>Here&rsquo;s what the man page for stan has to say:</p>
<blockquote>
<p>&ldquo;7. When an element is of type ‘list’, it is supposed to make it easier to pass data for those declared in Stan code such as ‘&ldquo;vector[J] y1[I]&quot;’ and ‘&ldquo;matrix[J,K] y2[I]&quot;’. Using the latter as an example, we can use a list for ‘y2’ if the list has &ldquo;I&rdquo; elements, each of which is an array (matrix) of dimension &ldquo;J<em>K&rdquo;. However, it is not possible to pass a list for data declared such as ‘&ldquo;vector[K] y3[I,J]&quot;’; the only way for it is to use an array with dimension &ldquo;I</em>J*K&rdquo;. In addition, technically a ‘data.frame’ in R is also a list, but it should not be used for the purpose here since a ‘data.frame’ will be converted to a matrix as described above.&rdquo;</p>
</blockquote>
<p>You can also assign something to the variable being declared in the declaration statement, e.g.</p>
<pre tabindex="0"><code>matrix[3, 2] a = 0.5 * (b + c);
</code></pre><p>This is extremely useful as a playground to understand how to initialize and define things in the language! For example, I just learned you can initialize things like so:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">
real x[3] = {1, 2, 3};
vector[3] y = [1, 2, 3]&#39;; // transpose necessary
row_vector[3] y = [1, 2, 3]; // no transpose necessary
matrix[3, 2] z = [[1, 2],
                  [3, 4],
                  [5, 6]]
</code></pre><p>This is <em>extremely</em> useful to understanding dimensions. The matrix is three row vectors, treated as a row vector? But what does it look like then?</p>
<p>Let&rsquo;s see section &ldquo;4.3. Vector, Matrix, and Array Expressions&rdquo; in the Stan manual.</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">matrix[3,2] m1 = [ [ 1, 2 ], [ 3, 4 ], [ 5, 6 ] ];
</code></pre><p>Hmmm&hellip;it says in the manual that &ldquo;A matrix expression consists of square brackets wrapped around a sequence of comma separated row vector expressions&rdquo;, so I suppose we can take that as a definition?</p>
<p>Array notation</p>
<pre tabindex="0"><code>int b[2, 3] = { { 1, 2, 3 },
                { 4, 5, 6 } };
</code></pre><p>no ragged arrays!</p>
<p>How do you select columns and rows and so forth? Like, if I have</p>
<p>vector[2] y[5]</p>
<p>what does y[,2] give me, versus y[2]? How does the answer change if I declare instead:</p>
<p>matrix[2,5] y</p>
<p>and what is a fast way to check these things?</p>
<h2 id="uses-of-generated-quantities">uses of generated quantities</h2>
<h3 id="model-summaries">model summaries</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>data(cars)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m &lt;- ulam(
</span></span><span style="display:flex;"><span>  alist(
</span></span><span style="display:flex;"><span>    dist ~ dnorm(mu,sigma),
</span></span><span style="display:flex;"><span>    mu &lt;- a + b*speed,
</span></span><span style="display:flex;"><span>    a ~ dnorm(0,100),
</span></span><span style="display:flex;"><span>    b ~ dnorm(0,10),
</span></span><span style="display:flex;"><span>    sigma ~ dexp(1)
</span></span><span style="display:flex;"><span>  ) , data=cars, iter = 2000, log_lik = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cars_model &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower = 1&gt; N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real&lt;lower = 0&gt; speed[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real&lt;lower = 0&gt; dist[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real&lt;lower = 0&gt; sigma;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real mu[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  a ~ normal(0, 1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  b ~ normal(0, 0.1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  sigma ~ exponential(1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  for (i in 1:N) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    mu[i] = a + b * speed[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  dist ~ normal(mu, sigma);  
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  target += normal_lpdf(dist | mu, sigma);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real mu[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real log_lik[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  for (i in 1:N) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    mu[i] = a + b * speed[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    log_lik[i] = normal_lpdf(dist[i] | mu[i], sigma);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  speed = cars$speed,
</span></span><span style="display:flex;"><span>  dist = cars$dist,
</span></span><span style="display:flex;"><span>  N = nrow(cars)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m &lt;- stan(model_code = cars_model,
</span></span><span style="display:flex;"><span>          data = dat_list, 
</span></span><span style="display:flex;"><span>          chains = 4,
</span></span><span style="display:flex;"><span>          iter = 4000,
</span></span><span style="display:flex;"><span>          control = list(adapt_delta=0.99))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>set.seed(94)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_samples &lt;- 8000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m, n = n_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># calculate manually from parameter posteriors</span>
</span></span><span style="display:flex;"><span>logprob &lt;- sapply( 1:n_samples , 
</span></span><span style="display:flex;"><span>  function(s) {
</span></span><span style="display:flex;"><span>    mu &lt;- post$a[s] + post$b[s] * cars$speed
</span></span><span style="display:flex;"><span>    dnorm( cars$dist , mu , post$sigma[s] , log=<span style="color:#00f">TRUE</span> )
</span></span><span style="display:flex;"><span>  })
</span></span><span style="display:flex;"><span>n_cases &lt;- nrow(cars)
</span></span><span style="display:flex;"><span>lppd &lt;- apply(logprob, 1, log_sum_exp) - log(n_samples)
</span></span><span style="display:flex;"><span>pWAIC &lt;- apply(logprob, 1, var)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># from the stanfit itself</span>
</span></span><span style="display:flex;"><span>lppd2 &lt;- apply(post$log_lik, 2, log_sum_exp) - log(n_samples)
</span></span><span style="display:flex;"><span>pWAIC2 &lt;- apply(post$log_lik, 2, var)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># logLik:</span>
</span></span><span style="display:flex;"><span>logLik(m)
</span></span><span style="display:flex;"><span>sum(lppd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># WAIC:</span>
</span></span><span style="display:flex;"><span>WAIC(m)
</span></span><span style="display:flex;"><span>k &lt;- 2
</span></span><span style="display:flex;"><span>-2 * sum(lppd) + k * sum(pWAIC)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># WAIC standard error:</span>
</span></span><span style="display:flex;"><span>sqrt(var(-2 * (lppd - pWAIC)) * n_cases)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># AIC:</span>
</span></span><span style="display:flex;"><span>AIC(m)
</span></span><span style="display:flex;"><span>k &lt;- 2
</span></span><span style="display:flex;"><span>-2 * sum(lppd) + k * length(coef(m))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># BIC</span>
</span></span><span style="display:flex;"><span>BIC(m)
</span></span><span style="display:flex;"><span>k &lt;- log(nrow(cars))
</span></span><span style="display:flex;"><span>-2 * sum(lppd) + k * length(coef(m))
</span></span></code></pre></div><p>We can have the model calculate and store functions of the posterior distribution on the outcome, such as deviance, using the &ldquo;generated quantities&rdquo; block. Here&rsquo;s an example with <code>surprise</code> (a.k.a. negative log-probability or negative-log-&ldquo;likelihood&rdquo;) and <code>deviance</code> for a bernoulli-logit model:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">generated quantities{
  vector[N] p;
  vector[N] surprise;
  vector[N] deviance;
  for (i in 1:N) {
    p[i] = a + b * x[i];
    surprise[i] = (-1) * binomial_logit_lpmf( y[i] | 1 , p[i] );
  }
  deviance = (-2) * binomial_logit_lpmf(y | 1 , p);
}
</code></pre><p>Given that there&rsquo;s a posterior on each <code>p[i]</code>, we must also have a posterior on each <code>log(1/p[i])</code> a.k.a. the surprise of observation <code>y[i]</code>. The deviance, on the other hand, is twice the total surprise over all outcome values in <code>y</code> and so is a single posterior.
z</p>
<h3 id="predicted-outcomes-on-existing-cases">predicted outcomes on existing cases</h3>
<p>from &ldquo;An easy way to simulate fake data from your Stan model&rdquo; by Jim Savage
<a href="http://modernstatisticalworkflow.blogspot.de/2017/04/an-easy-way-to-simulate-fake-data-from.html">http://modernstatisticalworkflow.blogspot.de/2017/04/an-easy-way-to-simulate-fake-data-from.html</a></p>
<p>His solution is to (1) modify the stan code to not evaluate the likelihood by adding an extra argument (run_estimation) and (2) include output in generated_quantities which comes from _rng functions on the parameters of the function</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower=1&gt; N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower = 0, upper = 1&gt; run_estimation; // a switch to evaluate the likelihood
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real y;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real x;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real&lt;lower=0&gt; sigma;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[N] mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  sigma ~ exponential( 1 );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  x ~ normal( 50 , 2 );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  b ~ normal( 0 , 1 );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  a ~ normal( 0 , 10 );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    mu[i] = a + b * x;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  if(run_estimation==1){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    y ~ normal( mu , sigma );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[N] mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[N] y_sim;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    mu[i] = a + b * x;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    y_sim[i] = normal_rng(mu[i], sigma);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>compiled_model &lt;- stan_model(model_code = model0)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sim_out &lt;- sampling(compiled_model, data = list(N = 1, run_estimation = 0), iter = 1000, chains = 1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fake_data_matrix  &lt;- sim_out %&gt;% 
</span></span><span style="display:flex;"><span>  as.data.frame %&gt;% 
</span></span><span style="display:flex;"><span>  select(contains(<span style="color:#a31515">&#34;y_sim&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>nrow(fake_data_matrix)
</span></span><span style="display:flex;"><span>dens(fake_data_matrix[,1])
</span></span></code></pre></div><h3 id="predicted-outcomes-on-new-cases">predicted outcomes on new cases</h3>
<p>Prepare Stan to receive existing data to fit and new data to predict by those fits at the same time:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/multivariate-linear-regression-pred.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>and a demonstration:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>intercept &lt;- rep(1, 1000)
</span></span><span style="display:flex;"><span>x1 &lt;- rnorm(1000, 0, 1)
</span></span><span style="display:flex;"><span>x2 &lt;- rnorm(1000, 0, 1)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(1000, 0.2 - (0.4 * x1) + (0.2 * x2), 0.2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x &lt;- as.matrix(cbind(intercept, x1, x2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>intercept_new &lt;- rep(10, 1)
</span></span><span style="display:flex;"><span>x1_new &lt;- rnorm(10, 0, 1)
</span></span><span style="display:flex;"><span>x2_new &lt;- rnorm(10, 0, 1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x_new &lt;- as.matrix(cbind(intercept_new, x1_new, x2_new))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  K = ncol(x),
</span></span><span style="display:flex;"><span>  x = x,
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  N = nrow(x),
</span></span><span style="display:flex;"><span>  x_new = x_new,
</span></span><span style="display:flex;"><span>  N_new = nrow(x_new)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(<span style="color:#a31515">&#34;../stan/multivariate-linear-regression-pred.stan&#34;</span>, data = dat_list, chains = 1)
</span></span></code></pre></div><h2 id="uses-of-transformed-parameters">uses of transformed parameters</h2>
<h3 id="models-with-missingness">Models with missingness</h3>
<p>Imagine a simple bivariate linear model with some missing values on your predictor <code>x</code>. We will define each missing value as a parameter in an <code>x_impute</code> parameter vector, and estimate them:</p>
<pre tabindex="0"><code>vector[N_x_missing] x_impute;
</code></pre><p>In <code>transformed parameters</code>, then, we assemble a <em>merged</em> predictor variable called <code>x_merge</code> which is what the model block sees, not <code>x</code> itself.</p>
<pre tabindex="0"><code>
transformed parameters{
  real x_merge[N_x];
  x_merge = x;
  for ( u in 1:N_x_miss ) x_merge[x_miss[u]] = x_impute[u];
}
</code></pre><p>There&rsquo;s one catch though - because <code>x_merge</code> contains things defined originally as parameters, it must have a prior defined in the model block, e.g.</p>
<pre tabindex="0"><code>  x_merge ~ normal(nu, sigma_x);
</code></pre><p>Imputation also requires some changes to the data. We need a benign value for missingness in <code>x</code> (<code>NA</code> is not acceptable), and a new data vector that indexes the missingness locations for the predictor, <code>x_miss</code>. Note that you <em>must</em> define integers that will be used as indexes explicitly with <code>int</code>:</p>
<pre tabindex="0"><code>int x_miss[N_x_miss]
</code></pre><p>Putting this all together, a simple imputation model is:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/simple-imputation.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Here&rsquo;s an example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NA_to_dummy &lt;- function(data){
</span></span><span style="display:flex;"><span>  data[is.na(data)] &lt;- (-99)
</span></span><span style="display:flex;"><span>  return(data)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>x &lt;- rnorm(N, 0, 1)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(N, 0.2 - (0.4 * x), 0.2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N_x_miss &lt;- 1000
</span></span><span style="display:flex;"><span>x_miss &lt;- sample(1:N, N_x_miss)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x[x_miss] &lt;- <span style="color:#00f">NA</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  x = NA_to_dummy(x),
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  N = length(x),
</span></span><span style="display:flex;"><span>  N_x_miss = N_x_miss,
</span></span><span style="display:flex;"><span>  x_miss = x_miss
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(<span style="color:#a31515">&#34;../stan/simple-imputation.stan&#34;</span>, data = dat_list, chains = 1)
</span></span></code></pre></div><p>This is great but does it really need to return the entire x_merge column??</p>
<p>NO! Better to simply include parameters for the distribtion&hellip;</p>
<p>Leads to an interesting questoin, hat happens if you pass it in w no missings?
It is fine! no problems at all&hellip;nu and sigma simply describe the observed data, no imputed values show up in the x_merged object</p>
<p>ok, what happens if you pass it in iwth ALL missings?
again, totally fine
nu and simga have nothign to fit to, so reflect their priors
with no info about x, we know nothing about b so that just reflects our prior</p>
<p>We can include predictor variables on the imputed values as well.</p>
<p>real x[N];
vector[N] y;
you cannot assign x = y, they are not the same type (somehow?)</p>
<p>Intresting&hellip;N_x versus N&hellip;it seems that the former prevents it from being returned? makes no sense&hellip;</p>
<p>The code seems to just replace the NA values with an address to empty memory, which probably means the value just doesn&rsquo;t get updated from the prior</p>
<p>interesting: when you have a vector of data representing a preidctor variable, you can assign it a probability function in the model block but that doesn&rsquo;t do anything if the data is observed right? if its missing it samples a new value from the probability function?</p>
<p>question: why arent all the imputed distribution values all <em>identical</em> in a model where they are all associated with the same probability distribution?</p>
<h3 id="modeling-the-missingness">Modeling the missingness</h3>
<p>To build a model of missingness, you make the missing parameter&rsquo;s prior a function of other variables. So</p>
<pre tabindex="0"><code>sigma_x ~ cauchy(0 , 1);
nu ~ normal(0.5 , 1);
x1_merge ~ normal(nu, sigma_x);
</code></pre><p>becomes</p>
<pre tabindex="0"><code>sigma_x ~ cauchy(0 , 1);
for ( i in 1:N ) {
  nu[i] = a_nu + g * x2[i];
}
x1_merge ~ normal(nu, sigma_x);
</code></pre><p>what happens if pass in data with no missings??</p>
<p>In practice, we are modeling the entire <code>x</code> variable, but the only time it matters is for the missing values.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>intercept &lt;- rep(1, N)
</span></span><span style="display:flex;"><span>x1 &lt;- rnorm(N, 0, 1)
</span></span><span style="display:flex;"><span>x2 &lt;- rnorm(N, 0, 1)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(N, 0.2 - (0.4 * x1) + (0.2 * x2), 0.2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N_x1_miss &lt;- 100
</span></span><span style="display:flex;"><span>x1_miss &lt;- sample(1:N, N_x1_miss)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x1[x1_miss] &lt;- <span style="color:#00f">NA</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NA_to_dummy &lt;- function(data){
</span></span><span style="display:flex;"><span>  data[is.na(data)] &lt;- (-99)
</span></span><span style="display:flex;"><span>  return(data)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  x1 = NA_to_dummy(x1),
</span></span><span style="display:flex;"><span>  x2 = x2,
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  N_x1_miss = N_x1_miss,
</span></span><span style="display:flex;"><span>  x1_miss = x1_miss
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m1 &lt;- stan(<span style="color:#a31515">&#34;../stan/modeled-imputation.stan&#34;</span>, data = dat_list, chains = 1)
</span></span></code></pre></div><p>Here&rsquo;s a neat q: assuming we know someting about the relationship between x1 and x2, can we estimate the effect of x1 if it is ENTIRELY missnng from the model but is imputed? This is like&hellip;IRT models!</p>
<pre tabindex="0"><code>y ~ normal(mu, sigma)
target += normal_rng(mu, sigma)
normal_lpmf(y | mu, sigma)
</code></pre><p>what are all the probability functions stan recognizes
and hwat rae their syntax??</p>
<p>section 50 in the stan manual starts listing the, and continues through section 64
after that its mostly style guides, so this relaly is the end of the manual</p>
<p>can you write your own?</p>
<p>is converting to target += illuminating?</p>
<p>I need to add priors on all the models above?
Whatabout doing that and disabling the sampling?</p>
<h2 id="quick-note-on-stan-efficiency">quick note on stan efficiency</h2>
<p>in exponentials, the stan parameter is the <em>rate</em>, e.g. 1/mu. If you set an exponential prior in the model block and give it the mu form, e.g. exponetial(1/0.5), it is <em>slower</em> than if you give it the rate form, e.g. exponential(2). Presumably this is because the calculation has to be done multiple times&hellip;</p>
<p>log-probability and gradient evaluation are the fundamental tick marks, called a &rsquo;leapfrog step&rsquo;
parameters and trasnformed parameers evaluated once per tick</p>
<p>generated quantities are defined once per <em>sample</em>, which occurs after all leapfrog steps have been completed</p>
<h2 id="notes-on-stan-evaluation-sequence">notes on stan evaluation sequence</h2>
<p>from section 6 of the manual, &ldquo;Overview of Stan&rsquo;s Program Blocks&rdquo;</p>
<p>The transformed data block is evaluated only <em>once</em>, so it is an ideal place to modify data from how it was fed in (e.g. for a bespoke task for a particular model)</p>
<p><em>declaration</em> and <em>statement</em>
the declaration sets the type and constraints
the statement defines how it works
declarations <em>must</em> come before statements for the same objects
the data and parameters blocks do not allow statements, only declarations. that&rsquo;s for the transformed data, and transformed paraemeters blocks</p>
<p>stan blocks are each optional but <em>must</em> occur in a specific order. also, things declared in later blocks generally dnt have scope in earlier ones, e.g. a variable declared in the generated quantities block can&rsquo;t be used in the model block. but they do have scope in later blocks, <em>except</em> things declared in the model block itself.</p>
<h1 id="2-probability-distributions">2. Probability Distributions</h1>
<p>(Eventually I should merge this with my probability notes)</p>
<h2 id="gaussian">gaussian</h2>
<p>Here&rsquo;s a basic univariate Gaussian model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/gaussian-model.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Note that this model is using <code>target +=</code> notation to specify the log-probability surface the Hamiltionian MCMC engine will simulate over. The Stan team calls this a &ldquo;log probability increment statement&rdquo;. We could also have written the outcome variable as a stochastic node, <code>y ~ normal(mu , sigma);</code>, a distribution statement, to get the same result. I don&rsquo;t know if there&rsquo;s ever any difference or why one or the other is better, I think Gelman just prefers the target version. The <code>target</code> is not a variable, just a reserved word that instructs Stan to add up the log-probabilities for each value of the right side. In general, it&rsquo;s the difference between</p>
<pre tabindex="0"><code>y ~ dist(theta1, ..., thetaN);
</code></pre><p>and</p>
<pre tabindex="0"><code>target += dist_lpdf(y | theta1, ..., thetaN);
</code></pre><p>for some named distribution, with either a LPDF or an LPMF.</p>
<p>Here&rsquo;s how you might use the Guassian model with real data:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- rnorm(100, 10, 2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> N = length(y)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower = 1&gt; N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real&lt;lower = 0&gt; sigma;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  mu ~ normal(0, 10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  sigma ~ exponential(1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">//  y ~ normal(mu , sigma);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  target += normal_lpdf(y | mu , sigma);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(model = model0, data = dat_list)
</span></span></code></pre></div><p>This works, but I don&rsquo;t like writing Stan models as a quoted string in R, because you can&rsquo;t get syntax highlighting. Better to store the Stan model in a seperate file ending in <code>.stan</code>, called like so:</p>
<pre tabindex="0"><code>
m0 &lt;- stan(file = &#34;../stan/gaussian-model.stan&#34;, data = dat_list)
</code></pre><h2 id="bernoulli">bernoulli</h2>
<p>Here&rsquo;s Bernoulli model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/bernoulli-model.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Note that we can use either a Binomial or Bernoulli logit LPMF. An example of it in use:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 100
</span></span><span style="display:flex;"><span>y &lt;- rbinom(N, 1, 0.3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> N = length(y)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/bernoulli-model.stan&#34;</span>, data = dat_list)
</span></span></code></pre></div><h2 id="categorical">categorical</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- sample(1:3, 100000, replace = <span style="color:#00f">TRUE</span>, prob = c(10, 2, 4))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> K = length(unique(y)),
</span></span><span style="display:flex;"><span> N = length(y),
</span></span><span style="display:flex;"><span> alpha = c(1, 1, 1)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>categorical &lt;- cmdstan_model(<span style="color:#a31515">&#34;stan/probability-distributions/categorical.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- categorical$sample(chains = 4, iter_warmup = 2000, iter_sampling = 4000, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- categorical$sample(chains = 1, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- as_draws_rvars(fit$draws())
</span></span><span style="display:flex;"><span>str(post) <span style="color:#008000"># can see what variables are in there!</span>
</span></span><span style="display:flex;"><span>theta &lt;- draws_of(post$theta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># thinkpad x390 - 470 seconds</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># tressa mac mini - 668 seconds</span>
</span></span></code></pre></div><h2 id="multi-logit-regression">multi-logit regression</h2>
<p>A.k.a multinomial regression&hellip;.basically just a categorical model with predictors. What&rsquo;s weird is that the predictors will act individually on the probability each outcome appears, but in a way that results in those probabilities adding to 1.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/multi-logit-regression.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>The special case, $K = 2$ outcomes. This is identical conceptually to a binomial model with a logistic link, so we should be able to analyze the same data both ways.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>log_sum_exp &lt;- function(x) {
</span></span><span style="display:flex;"><span>    x_c &lt;- x - max(x)
</span></span><span style="display:flex;"><span>    out &lt;- max(x) + log(sum(exp(x_c)))
</span></span><span style="display:flex;"><span>    return(out)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>softmax &lt;- function(x) exp(x - log_sum_exp(x))
</span></span><span style="display:flex;"><span><span style="color:#008000"># in reality, softmax(x) = exp(x) / sum(exp(x))</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># generate from a binomial with one predictor</span>
</span></span><span style="display:flex;"><span>logistic &lt;- function(x) exp(x)/(1 + exp(x))
</span></span><span style="display:flex;"><span>N &lt;- 2000
</span></span><span style="display:flex;"><span>male &lt;- rbinom(N, 1, 0.5)
</span></span><span style="display:flex;"><span>a &lt;- logit(0.3)
</span></span><span style="display:flex;"><span>b &lt;- 1
</span></span><span style="display:flex;"><span>y &lt;- rbinom(N, 1, prob = logistic(a * 1 + b * male))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  K = length(unique(y)),
</span></span><span style="display:flex;"><span>  N = length(y),
</span></span><span style="display:flex;"><span>  x = male
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/logistic-regression-model.stan&#34;</span>, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># generate from multinomial with one predictor</span>
</span></span><span style="display:flex;"><span>N &lt;- 2000
</span></span><span style="display:flex;"><span>D &lt;- 2 <span style="color:#008000"># number of &#39;predictors&#39;</span>
</span></span><span style="display:flex;"><span>K &lt;- 2 <span style="color:#008000"># number of outcome categories</span>
</span></span><span style="display:flex;"><span>X &lt;- matrix(<span style="color:#00f">NA</span>, nrow = N, ncol = D)
</span></span><span style="display:flex;"><span><span style="color:#008000"># or, an array of size N, each element of which is a vector of length D</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># in a logit the first preditor is just the intercept, so 1</span>
</span></span><span style="display:flex;"><span>male &lt;- rbinom(N, 1, 0.5)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X[,1] &lt;- 1
</span></span><span style="display:flex;"><span>X[,2] &lt;- male
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>beta &lt;- matrix(<span style="color:#00f">NA</span>, nrow = K, ncol = D)
</span></span><span style="display:flex;"><span>beta[, 1] &lt;- c(logit(0.3), logit(0.7))
</span></span><span style="display:flex;"><span>beta[, 2] &lt;- c(1, -1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pr_outcome &lt;- matrix(<span style="color:#00f">NA</span>, nrow = K, ncol = N)
</span></span><span style="display:flex;"><span>y &lt;- rep(<span style="color:#00f">NA</span>, length = N)
</span></span><span style="display:flex;"><span>for (i in 1:N) {
</span></span><span style="display:flex;"><span>  pr_outcome[,i] &lt;- softmax(beta %*% X[i,])
</span></span><span style="display:flex;"><span>  y[i] &lt;- sample(c(1, 2), 1, prob = pr_outcome[,i])
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  K = K,
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  D = D,
</span></span><span style="display:flex;"><span>  x = X
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m1 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/multi-logit-regression-efficient.stan&#34;</span>, data = dat_list)
</span></span></code></pre></div><p>I don&rsquo;t think it&rsquo;s converging correctly? Hmm&hellip;.</p>
<p>The general case, where there are more than 2 outcomes, requires us to either appeal to regularization to identify parameters.</p>
<h2 id="poisson">poisson</h2>
<p>Here&rsquo;s a Poisson model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/poisson-model.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>and an example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- rpois(1000, 2.3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> N = length(y)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/poisson-model.stan&#34;</span>, data = dat_list)
</span></span></code></pre></div><h2 id="exponential">exponential</h2>
<p>Here&rsquo;s an Exponential model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/exponential-model.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>and an example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- rexp(1000, 2.3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> N = length(y)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/exponential-model.stan&#34;</span>, data = dat_list)
</span></span></code></pre></div><h2 id="binomial">binomial</h2>
<p>Here&rsquo;s a Binomial:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/binomial-model.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>and an example</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N_trials &lt;- 10
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- rbinom(1000, N_trials, 0.3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> N = length(y),
</span></span><span style="display:flex;"><span> N_trials = N_trials
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/binomial-model.stan&#34;</span>, data = dat_list)
</span></span></code></pre></div><h2 id="geometric">geometric</h2>
<p>Stan does not define the Geometric probability functions explicitly. However, it&rsquo;s easy enough to write them out in terms of Bernoullis. For example, if Y ~ Geom(p), then the log-PMF is</p>
<p>$$
log(Pr(Y = y)) = log(p) + y * log((1-p)),
$$</p>
<p>which we can use to update the <code>target</code> in Stan.</p>
<p>And an example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- rgeom(1000, 0.3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span> y = y,
</span></span><span style="display:flex;"><span> N = length(y)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(file = <span style="color:#a31515">&#34;../stan/geometric-model.stan&#34;</span>, data = dat_list)
</span></span></code></pre></div><h2 id="what-is-the-right-way-to-summarize-a-lognormal">what is the right way to summarize a lognormal?</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 100000
</span></span><span style="display:flex;"><span>set.seed(6)
</span></span><span style="display:flex;"><span>mu &lt;- rnorm(1, 0, 1)
</span></span><span style="display:flex;"><span>sigma &lt;- rexp(1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># graph true values</span>
</span></span><span style="display:flex;"><span>curve(dlnorm(x, mu, sigma), xlim = c(0, 3))
</span></span><span style="display:flex;"><span>abline(v = exp(mu)) <span style="color:#008000"># median aka geom. mean</span>
</span></span><span style="display:flex;"><span>abline(v = exp(mu + sigma^2/2)) <span style="color:#008000"># mean</span>
</span></span><span style="display:flex;"><span>y &lt;- rlnorm(N, mu, sigma)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(y) <span style="color:#008000"># 3.48</span>
</span></span><span style="display:flex;"><span>median(y) <span style="color:#008000"># 1.96</span>
</span></span><span style="display:flex;"><span>exp(mean(log(y))) <span style="color:#008000"># 1.95</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  y = y
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- models$lognormal$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(posterior)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>samples &lt;- as.data.frame(as_draws_df(fit$draws()))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>median_y_mean &lt;- mean(exp(samples$mu))
</span></span><span style="display:flex;"><span>median_y_lb &lt;- HPDI(exp(samples$mu))[1]
</span></span><span style="display:flex;"><span>median_y_ub &lt;- HPDI(exp(samples$mu))[2]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean_y_mean &lt;- mean(exp(samples$mu + samples$sigma^2/2))
</span></span><span style="display:flex;"><span>mean_y_lb &lt;- HPDI(exp(samples$mu + samples$sigma^2/2))[1]
</span></span><span style="display:flex;"><span>mean_y_ub &lt;- HPDI(exp(samples$mu + samples$sigma^2/2))[2]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>points(median_y_mean, dlnorm(exp(mu), mu, sigma), col = <span style="color:#a31515">&#34;blue&#34;</span>)
</span></span><span style="display:flex;"><span>lines(c(median_y_lb, median_y_ub), c(dlnorm(exp(mu), mu, sigma), dlnorm(exp(mu), mu, sigma)), col = <span style="color:#a31515">&#34;blue&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>points(mean_y_mean, dlnorm(exp(mu + sigma^2/2), mu, sigma), col = <span style="color:#a31515">&#34;blue&#34;</span>)
</span></span><span style="display:flex;"><span>lines(c(mean_y_lb, mean_y_ub), c(dlnorm(exp(mu + sigma^2/2), mu, sigma), dlnorm(exp(mu + sigma^2/2), mu, sigma)), col = <span style="color:#a31515">&#34;blue&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#008000"># boo ya!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># is a normal faster?</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  y = log(y)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- models$gaussian$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>samples &lt;- as.data.frame(as_draws_df(fit$draws()))
</span></span><span style="display:flex;"><span>median_y_mean &lt;- mean(exp(samples$mu))
</span></span><span style="display:flex;"><span>median_y_lb &lt;- HPDI(exp(samples$mu))[1]
</span></span><span style="display:flex;"><span>median_y_ub &lt;- HPDI(exp(samples$mu))[2]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean_y_mean &lt;- mean(exp(samples$mu + samples$sigma^2/2))
</span></span><span style="display:flex;"><span>mean_y_lb &lt;- HPDI(exp(samples$mu + samples$sigma^2/2))[1]
</span></span><span style="display:flex;"><span>mean_y_ub &lt;- HPDI(exp(samples$mu + samples$sigma^2/2))[2]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># it works</span>
</span></span><span style="display:flex;"><span>points(median_y_mean, dlnorm(exp(mu), mu, sigma), col = <span style="color:#a31515">&#34;red&#34;</span>)
</span></span><span style="display:flex;"><span>lines(c(median_y_lb, median_y_ub), c(dlnorm(exp(mu), mu, sigma), dlnorm(exp(mu), mu, sigma)), col = <span style="color:#a31515">&#34;red&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>points(mean_y_mean, dlnorm(exp(mu + sigma^2/2), mu, sigma), col = <span style="color:#a31515">&#34;red&#34;</span>)
</span></span><span style="display:flex;"><span>lines(c(mean_y_lb, mean_y_ub), c(dlnorm(exp(mu + sigma^2/2), mu, sigma), dlnorm(exp(mu + sigma^2/2), mu, sigma)), col = <span style="color:#a31515">&#34;red&#34;</span>)
</span></span></code></pre></div><h2 id="custom-probability-distributions">custom probability distributions</h2>
<p>Since you just need to update the target log-probability, anything that is a valid PDF can be supplied, if you want to create custom models!</p>
<h1 id="3---regression-models">3 - regression models</h1>
<p>A Gaussian model with a predictor variable determining <code>mu</code> for each case might look like this:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/simple-linear-regression.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Note that we either could use a <code>for</code> loop or vectorized notation to calculate <code>mu</code>. Now compare with the Gaussian from before:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/gaussian-model.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Note that we define <code>mu</code> is now a vector, and is defined inside the the model block rather than as a parameter. An example in use:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x &lt;- rnorm(1000, 0, 1)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(1000, 0.2 - (0.4 * x), 0.2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  x = x,
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  N = length(x)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(<span style="color:#a31515">&#34;../stan/simple-linear-regression.stan&#34;</span>, data = dat_list, chains = 1)
</span></span></code></pre></div><p>A note on notation. Compare these initializations of <code>mu</code>:</p>
<pre tabindex="0"><code>  real mu[N];
  vector[N] mu; 
</code></pre><p>Both seem to acomplish the same thing - create a vector of real numbers of length <code>N</code> and call it <code>mu</code> - but this is not so. They will work in <code>for</code> loops equally well:</p>
<pre tabindex="0"><code>  for ( i in 1:N ) {
    mu[i] = a + (b * x[i]);
  }
</code></pre><p>but only the vector will work in vectorize operations, e.g.</p>
<pre tabindex="0"><code>  mu = a + b * x;
</code></pre><p>Extending to multiple predictors is straightforwards, provided we use matrix notation:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(readLines(<span style="color:#a31515">&#34;../stan/multivariate-linear-regression.stan&#34;</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>In addition to the parameter fits, here I&rsquo;ve included a &ldquo;generated quantities&rdquo; block that simulates outcomes for each case using the <code>normal_rng</code> function, creating posterior prediction distributions. See it in action:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x1 &lt;- rnorm(1000, 0, 1)
</span></span><span style="display:flex;"><span>x2 &lt;- rnorm(1000, 0, 1)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(1000, 0.2 - (0.4 * x1) + (0.2 * x2), 0.2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>intercept &lt;- rep(1, 1000)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x &lt;- as.matrix(cbind(intercept, x1, x2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  K = ncol(x),
</span></span><span style="display:flex;"><span>  x = x,
</span></span><span style="display:flex;"><span>  y = y,
</span></span><span style="display:flex;"><span>  N = nrow(x)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- stan(<span style="color:#a31515">&#34;../stan/multivariate-linear-regression.stan&#34;</span>, data = dat_list, chains = 1)
</span></span></code></pre></div><p>The fit now has a parameter <code>y_pred[i]</code> for the predicted outcome of each case <code>i</code>.</p>
<h1 id="4---hierarchicial-models">4 - hierarchicial models</h1>
<h2 id="varying-and-fixed-effects-models">varying and fixed effects models</h2>
<p>(soon)</p>
<h2 id="basic-operations">basic operations</h2>
<p><code>real a[2,3]</code> and <code>matrix[2, 3] b</code> seem to produce identical structures? ah, but there are differences. <code>b</code> can be assigned a standard prior using the <code>to_vector(b)</code> operation, <code>a</code> cannot. in both, you can assign priors to each row as a vector inside a for-loop, but only the matrix can take <code>to_vector</code>.</p>
<p>You <em>must</em> use <code>to_vector</code> on a matrix, even if the matrix is itself 1xk or kx1</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>source(<span style="color:#a31515">&#34;project_support.r&#34;</span>); models$declaration_practice$sample(chains = 1, iter_warmup = 20, iter_sampling = 20)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>source(<span style="color:#a31515">&#34;project_support.r&#34;</span>); models$distribution_practice$sample(chains = 1, iter_warmup = 20, iter_sampling = 20)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- models$declaration_practice$sample(chains = 1, iter_warmup = 20, iter_sampling = 20)
</span></span><span style="display:flex;"><span>samples &lt;- as.data.frame(as_draws_df(fit$draws()))
</span></span></code></pre></div><h2 id="correlated-varying-effects">correlated varying effects</h2>
<p>one of the main advantages of varying effects models is that it allows us to specify joint distributions using, e.g. multivariate normal distribution. The <code>multilevel_regression_rho</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(mvtnorm)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>diag_matrix &lt;- function(v) {
</span></span><span style="display:flex;"><span>  out &lt;- diag(length(v))
</span></span><span style="display:flex;"><span>  diag(out) &lt;- v
</span></span><span style="display:flex;"><span>  return(out)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>quad_form_diag &lt;- function(Omega, tau) {
</span></span><span style="display:flex;"><span>  diag_matrix(tau) %*% Omega %*% diag_matrix(tau)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>set.seed(5)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars &lt;- list(
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># sim parameters</span>
</span></span><span style="display:flex;"><span>  N_obs = 20,
</span></span><span style="display:flex;"><span>  N_grp = 10,
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># prior parameters</span>
</span></span><span style="display:flex;"><span>  a_pop_mu = 0,
</span></span><span style="display:flex;"><span>  a_pop_sigma = 1,
</span></span><span style="display:flex;"><span>  a_grp_sigma_mu = 1,
</span></span><span style="display:flex;"><span>  b_pop_mu = 1,
</span></span><span style="display:flex;"><span>  b_pop_sigma = 1,
</span></span><span style="display:flex;"><span>  b_grp_sigma_mu = 1,
</span></span><span style="display:flex;"><span>  sigma_mu = 1,
</span></span><span style="display:flex;"><span>  ab_grp_eta = 2
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars$id &lt;- sample(1:pars$N_grp, pars$N_obs, replace = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>pars$a_grp_sigma &lt;- rexp(1, 1/pars$a_grp_sigma_mu)
</span></span><span style="display:flex;"><span>pars$b_grp_sigma &lt;- rexp(1, 1/pars$b_grp_sigma_mu)
</span></span><span style="display:flex;"><span>pars$a_pop &lt;- rnorm(1, pars$a_pop_mu, pars$a_pop_sigma)
</span></span><span style="display:flex;"><span>pars$b_pop &lt;- rnorm(1, pars$b_pop_mu, pars$b_pop_sigma)
</span></span><span style="display:flex;"><span>pars$ab_mu &lt;- c(0, 0)
</span></span><span style="display:flex;"><span>ab_grp_rho &lt;- 2 * rbeta(1, pars$ab_grp_eta, pars$ab_grp_eta) - 1 <span style="color:#008000"># gotta rescale!</span>
</span></span><span style="display:flex;"><span>pars$ab_grp_Omega &lt;- matrix(c(1, ab_grp_rho, ab_grp_rho, 1), ncol = 2)
</span></span><span style="display:flex;"><span>ab_grp &lt;- rmvnorm(pars$N_grp, pars$ab_mu, pars$ab_grp_Omega)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a &lt;- pars$a_pop + ab_grp[,1] * pars$a_grp_sigma
</span></span><span style="display:flex;"><span>b &lt;- pars$b_pop + ab_grp[,2] * pars$b_grp_sigma
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars$sigma &lt;- rexp(1, 1/pars$sigma_mu)
</span></span><span style="display:flex;"><span>pars$x &lt;- rnorm(pars$N_obs, 0, 1)
</span></span><span style="display:flex;"><span>mu &lt;- a[pars$id] + b[pars$id] * pars$x
</span></span><span style="display:flex;"><span>pars$y &lt;- rnorm(pars$N_obs, mu, pars$sigma)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cor(ab_grp[,1], ab_grp[,2]) <span style="color:#008000"># this is what we estimate!</span>
</span></span><span style="display:flex;"><span>plot(ab_grp[,1], ab_grp[,2])
</span></span><span style="display:flex;"><span>ab_grp_rho
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># fit_hierarchical_test &lt;- models$multilevel_regression$sample(parallel_chains = 3, chains = 3,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   max_treedepth = 10, data = pars, step_size = 0.1, seed = 1,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   refresh = 0</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># )</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># fit_hierarchical_test &lt;- models$multilevel_regression_rho$sample(parallel_chains = 3, chains = 3,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   max_treedepth = 10, data = pars, step_size = 0.1, seed = 1,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   refresh = 0</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># )</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_hierarchical_test &lt;- models$multilevel_regression_rho_alt$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = pars, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0, output_dir = <span style="color:#a31515">&#34;.&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_hierarchical_test$save_object(file = <span style="color:#a31515">&#34;test.RDS&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x &lt;- readRDS(<span style="color:#a31515">&#34;test.RDS&#34;</span>)
</span></span><span style="display:flex;"><span>post &lt;- as_draws_rvars(x$draws())
</span></span><span style="display:flex;"><span>str(post)
</span></span><span style="display:flex;"><span>apply(draws_of(post$a), 2, mean) <span style="color:#008000"># works!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- as_draws_rvars(fit_hierarchical_test$draws())
</span></span><span style="display:flex;"><span>str(post) <span style="color:#008000"># can see what variables are in there!</span>
</span></span><span style="display:flex;"><span>a_post &lt;- draws_of(post$a)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>samples &lt;- as.data.frame(as_draws_df(fit_hierarchical_test$draws()))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dens(samples$ab_grp_rho, xlim = c(-1, 1))
</span></span><span style="display:flex;"><span>abline(v = ab_grp_rho, lty = 2)
</span></span><span style="display:flex;"><span>abline(v = cor(ab_grp[,1], ab_grp[,2]), col = <span style="color:#a31515">&#34;blue&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># fit_hierarchical_test &lt;- models$multilevel_regression_rho_cholesky$sample(parallel_chains = 3, chains = 3,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   max_treedepth = 10, data = pars, step_size = 0.1, seed = 1,</span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#   refresh = 0</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># )</span>
</span></span></code></pre></div><p>It looks like I figured it out! but i gotta rewrite the other versions to conform to my new simulation names&hellip;</p>
<h2 id="mixture-models">mixture models</h2>
<p>Let&rsquo;s do a zero-inflated Poisson!</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>theta &lt;- 0.26
</span></span><span style="display:flex;"><span>lambda &lt;- 0.9
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 5000
</span></span><span style="display:flex;"><span>y_true &lt;- rpois(N, lambda)
</span></span><span style="display:flex;"><span>y &lt;- y_true * rbinom(N, 1, (1 - theta))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  y = y
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_stan &lt;- cmdstan_model(<span style="color:#a31515">&#34;stan/simple-mixtures/zero-inflated-poisson.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_stan &lt;- m_stan$sample(
</span></span><span style="display:flex;"><span>  data = stan_data, 
</span></span><span style="display:flex;"><span>  seed = 123, 
</span></span><span style="display:flex;"><span>  chains = 4, 
</span></span><span style="display:flex;"><span>  parallel_chains = 4,
</span></span><span style="display:flex;"><span>  refresh = 500
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_stan$output_files() |&gt;
</span></span><span style="display:flex;"><span>  rstan::read_stan_csv() |&gt;
</span></span><span style="display:flex;"><span>  extract.samples() -&gt; samples
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(samples$theta)
</span></span><span style="display:flex;"><span>mean(samples$lambda)
</span></span><span style="display:flex;"><span><span style="color:#008000"># works!</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># these are NOT the same</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x &lt;- 0
</span></span><span style="display:flex;"><span>x &lt;- x + log_sum_exp(c(dbern(1, p, log = <span style="color:#00f">TRUE</span>), dbern(0, p, log = <span style="color:#00f">TRUE</span>) + dpois(0, lambda, log = <span style="color:#00f">TRUE</span>)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- 0
</span></span><span style="display:flex;"><span>y &lt;- y + dbern(1, p, log = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>y &lt;- y + dbern(0, p, log = <span style="color:#00f">TRUE</span>) + dpois(0, lambda, log = <span style="color:#00f">TRUE</span>)
</span></span></code></pre></div><h1 id="5---multivariate-outcome-models">5 - multivariate outcome models</h1>
<p>A vector $X$ that follows a multivariate normal has a covariance matrix $\Sigma$. We can generate multivariate gaussian data from</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(mvtnorm)
</span></span><span style="display:flex;"><span>s &lt;- matrix(c(3, 1.9, 1.9, 2), ncol = 2, byrow = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>d &lt;- rmvnorm(10000, mean = c(0, 0), sigma = s)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cov(d[,1], d[,2]) <span style="color:#008000"># 1.9</span>
</span></span><span style="display:flex;"><span>var(d[,1]) <span style="color:#008000"># 2.9</span>
</span></span><span style="display:flex;"><span>var(d[,2]) <span style="color:#008000"># 2.0</span>
</span></span></code></pre></div><p>The basic problem with covariance matricies is that we have to incorporate a lot of constraints into $\Sigma$: the diagonals must be the <em>squares</em> of the quantities we care about, and no off-diagonal entry can be larger than the product of the corresponding diagonal pair. Not fun for automatic generation or estimation.</p>
<p>Instead, we can decompose $\Sigma$ into a standard deviation vector (which has no constraints, other than positive reals) and correlation matrix (which has standard constraints), and let matrix algebra produce the resulting $\Sigma$. For two variables,</p>
<h1 id="endbmatrix">$$
\begin{bmatrix}
\sigma_1^2 &amp; \rho\sigma_1\sigma_2\
\rho\sigma_1\sigma_2 &amp; \sigma_2^2
\end{bmatrix}</h1>
<p>\begin{bmatrix}
\sigma_1 &amp; 0\
0 &amp; \sigma_2
\end{bmatrix}
\begin{bmatrix}
1 &amp; \rho\
\rho &amp; 1
\end{bmatrix}
\begin{bmatrix}
\sigma_1 &amp; 0\
0 &amp; \sigma_2
\end{bmatrix}
$$</p>
<p>More generally,</p>
<p>$$
\mathrm{Cov}(X) = \mathrm{diag}(\Sigma)^{1/2}; \mathrm{Cor}(X);\mathrm{diag}(\Sigma)^{1/2}
$$</p>
<p>In R, we can take advantage of some wrapper functions for these operations, which have equivalent functions in Stan:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>diag_matrix &lt;- function(v) {
</span></span><span style="display:flex;"><span>  out &lt;- diag(length(v))
</span></span><span style="display:flex;"><span>  diag(out) &lt;- v
</span></span><span style="display:flex;"><span>  return(out)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>quad_form_diag &lt;- function(Omega, tau) {
</span></span><span style="display:flex;"><span>  diag_matrix(tau) %*% Omega %*% diag_matrix(tau)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tau &lt;- c(3, 2) <span style="color:#008000"># sigma</span>
</span></span><span style="display:flex;"><span>Omega &lt;- matrix(c(1, 0.31415, 0.31415, 1), ncol = 2, byrow = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>s &lt;- quad_form_diag(Omega, tau)
</span></span><span style="display:flex;"><span>mu &lt;- rnorm(2, 0, 1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(mvtnorm)
</span></span><span style="display:flex;"><span>d &lt;- rmvnorm(1000, mean = mu, sigma = s)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cor(d[,1], d[,2]) <span style="color:#008000"># 0.5</span>
</span></span><span style="display:flex;"><span>var(d[,1]) <span style="color:#008000"># 9.1</span>
</span></span><span style="display:flex;"><span>var(d[,2]) <span style="color:#008000"># 4.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  K = 2,
</span></span><span style="display:flex;"><span>  N = 1000,
</span></span><span style="display:flex;"><span>  y = d,
</span></span><span style="display:flex;"><span>  mu_mu = 0,
</span></span><span style="display:flex;"><span>  mu_sigma = 1,
</span></span><span style="display:flex;"><span>  tau_rate = 0.4,
</span></span><span style="display:flex;"><span>  eta = 2
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tic()
</span></span><span style="display:flex;"><span>fit_multivariate_test &lt;- cmdstan_models$multivariate_test$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>toc()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post_array &lt;- fit_multivariate_test$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>sd(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span></code></pre></div><p>Of course, in Stan the <code>tau</code> and <code>Omega</code> objects are random parameters we assign priors to, so generatively we should also use random</p>
<p>The Stan manual recommends giving the correlation matrix an LKJ prior, with shape $\nu \geq 1$. We can also generate correlation matricies from the <code>rlkjcorr</code> function, which Richard included in <code>rethinking</code> with the following description:</p>
<pre tabindex="0"><code>The LKJ correlation matrix distribution is based upon work by Lewandowski, Kurowicka, and Joe. When the parameter ‘eta’ is equal to 1, it defines a flat distribution of correlation matrices. When ‘eta &gt; 1’, the distribution is instead concentrated towards to identity matrix. When ‘eta &lt; 1’, the distribution is more concentrated towards extreme correlations at -1 or +1.

It can be easier to understand this distribution if we recognize that the individual correlations within the matrix follow a beta distribution defined on -1 to +1. Thus ‘eta’ resembles ‘theta’ in the beta parameterization with a mean p and scale (sample size) theta.

The function ‘rlkjcorr’ returns an 3-dimensional array in which the first dimension indexes matrices. In the event that ‘n=1’, it returns instead a single matrix.
</code></pre><p>If we use Richard&rsquo;s rlkjcorr function, we can make an aribtrary number of variables.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>K &lt;- 2
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>mu_mu &lt;- 0
</span></span><span style="display:flex;"><span>mu_sigma &lt;- 1
</span></span><span style="display:flex;"><span>eta &lt;- 3
</span></span><span style="display:flex;"><span>tau_rate &lt;- 0.4
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tau &lt;- rexp(K, tau_rate)
</span></span><span style="display:flex;"><span>Omega &lt;- rethinking::rlkjcorr(n = 1, K = K, eta = eta)
</span></span><span style="display:flex;"><span>s &lt;- quad_form_diag(Omega, tau)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(mvtnorm)
</span></span><span style="display:flex;"><span>d &lt;- rmvnorm(N, mean = rnorm(K, mu_mu, mu_sigma), sigma = s)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cor(d[,1], d[,2]) <span style="color:#008000"># Omega[1,2], the true generative correlation</span>
</span></span><span style="display:flex;"><span>var(d[,1]) <span style="color:#008000"># tau[1]^2</span>
</span></span><span style="display:flex;"><span>var(d[,2]) <span style="color:#008000"># tau[2]^2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  K = K,
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  y = d,
</span></span><span style="display:flex;"><span>  mu_mu = mu_mu,
</span></span><span style="display:flex;"><span>  mu_sigma = mu_sigma,
</span></span><span style="display:flex;"><span>  tau_rate = tau_rate,
</span></span><span style="display:flex;"><span>  eta = eta
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tic(<span style="color:#a31515">&#34;fit multivariate&#34;</span>)
</span></span><span style="display:flex;"><span>fit_multivariate_test &lt;- cmdstan_models$multivariate_test$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>toc()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post_array &lt;- fit_multivariate_test$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>sd(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tic(<span style="color:#a31515">&#34;fit cholesky&#34;</span>)
</span></span><span style="display:flex;"><span>fit_multivariate_cholesky_test &lt;- cmdstan_models$multivariate_cholesky_test$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>toc()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post_array &lt;- fit_multivariate_cholesky_test$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(post[,<span style="color:#a31515">&#34;L_Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>sd(post[,<span style="color:#a31515">&#34;L_Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># half-cauchy instead of exponential? page 156</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># is this really optimized tho? bingo!</span>
</span></span></code></pre></div><p>The cholesky version seems to take about half the time, wow. But how to compute the actual correlations? Generated quantities? Yes, there&rsquo;s no real computational cost.</p>
<p>However, there is some optimization to be done, ok at the stan manual beginning page 147</p>
<blockquote>
<p>Optimization through Cholesky Factorization - The multivariate normal density and LKJ prior on correlation matrices both require their matrix parameters to be factored. Vectorizing, as in the previous section, ensures this is only done once for each density. An even better solution, both in terms of efficiency and numerical stability, is to parameterize the model directly in terms of Cholesky factors of correlation matrices using the multivariate version of the non-centered parameterization. For the model in the previous section, the program fragment to replace the full matrix prior with an equivalent Cholesky factorized prior is as follows.</p>
</blockquote>
<p>The <code>lkj_corr_cholesky</code> distribution is an implicit distribution, where</p>
<pre tabindex="0"><code>L ~ lkj_corr_cholesky(2.0);
# implies L * L&#39; ~ lkj_corr(2.0);
</code></pre><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>diag_pre_multiply &lt;- function(tau, Omega) {
</span></span><span style="display:flex;"><span>  diag_matrix(tau) %*% Omega
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>K &lt;- 2
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tau &lt;- rexp(2, 1)
</span></span><span style="display:flex;"><span>Omega &lt;- rethinking::rlkjcorr(n = 1, K = 2, eta = 3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>L_Omega &lt;- chol(Omega)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># t(L_Omega) %*% L_Omega returns Omega</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>s &lt;- (diag_pre_multiply(tau, L_Omega) %*% t(diag_pre_multiply(tau, L_Omega)))
</span></span><span style="display:flex;"><span><span style="color:#008000"># s &lt;- quad_form_diag(Omega, tau) is equivalent</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># the trick is tat Omega is never instantiated inside the Stan pogram, you specify L_Omega only!</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># i am not quite sure why that is faster but I can investigate...</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>K &lt;- 2
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>mu_mu &lt;- 0
</span></span><span style="display:flex;"><span>mu_sigma &lt;- 1
</span></span><span style="display:flex;"><span>eta &lt;- 3
</span></span><span style="display:flex;"><span>tau_rate &lt;- 0.4
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tau &lt;- rexp(K, tau_rate)
</span></span><span style="display:flex;"><span>Omega &lt;- rethinking::rlkjcorr(n = 1, K = K, eta = eta)
</span></span><span style="display:flex;"><span>s &lt;- quad_form_diag(Omega, tau)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(mvtnorm)
</span></span><span style="display:flex;"><span>d &lt;- rmvnorm(N, mean = rnorm(K, mu_mu, mu_sigma), sigma = s)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cor(d[,1], d[,2]) <span style="color:#008000"># Omega[1,2], the true generative correlation</span>
</span></span><span style="display:flex;"><span>var(d[,1]) <span style="color:#008000"># tau[1]^2</span>
</span></span><span style="display:flex;"><span>var(d[,2]) <span style="color:#008000"># tau[2]^2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  K = K,
</span></span><span style="display:flex;"><span>  N = N,
</span></span><span style="display:flex;"><span>  y = d,
</span></span><span style="display:flex;"><span>  mu_mu = mu_mu,
</span></span><span style="display:flex;"><span>  mu_sigma = mu_sigma,
</span></span><span style="display:flex;"><span>  tau_rate = tau_rate,
</span></span><span style="display:flex;"><span>  eta = eta
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tic(<span style="color:#a31515">&#34;fit multivariate&#34;</span>)
</span></span><span style="display:flex;"><span>fit_multivariate_test &lt;- cmdstan_models$multivariate_test$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>toc()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post_array &lt;- fit_multivariate_test$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>sd(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tic(<span style="color:#a31515">&#34;fit cholesky&#34;</span>)
</span></span><span style="display:flex;"><span>fit_multivariate_cholesky_test &lt;- cmdstan_models$multivariate_cholesky_test$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 0
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>toc()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post_array &lt;- fit_multivariate_cholesky_test$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>sd(post[,<span style="color:#a31515">&#34;Omega[2,1]&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># half-cauchy instead of exponential? page 156</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># is this really optimized tho? bingo!</span>
</span></span></code></pre></div><h2 id="some-notes-on-lk_rho">some notes on lk_rho</h2>
<p>If I am trying to estimate this from data, it has to be assigned a prior, and so it has to be generated stochastically inside the simulator! the implication is that i need to set a rho_mu and rho_sigma and use a truncated normal or similar&hellip;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lk_rho_mu &lt;- 0.7
</span></span><span style="display:flex;"><span>lk_rho_theta &lt;- 10
</span></span><span style="display:flex;"><span>lk_rho &lt;- rbeta(1, lk_rho_mu * lk_rho_theta, (1 - lk_rho_mu) * lk_rho_theta)
</span></span><span style="display:flex;"><span>lk_rho_Omega &lt;- matrix(c(1, lk_rho, lk_rho, 1), ncol = 2)
</span></span><span style="display:flex;"><span>lk_rho_L_Omega &lt;- chol(lk_rho_Omega)
</span></span><span style="display:flex;"><span>lk_rho_Omega
</span></span><span style="display:flex;"><span>chol(lk_rho_Omega) <span style="color:#008000"># so the upper-right is the rho, the lower-right is what exactly?</span>
</span></span></code></pre></div><p>what does eta do</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 0.5)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># backed up on -1 and 1, lowest at 0.5</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 1.0)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># uniform between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 1.5)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but quite dispersed between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 2)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but quite dispersed between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 5)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but mildly dispersed between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 10)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but a little dispersed between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 20)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but very little dispersed between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 50)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but very very little dispersed between -1 and 1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 100)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, but very very very little dispersed between -1</span>
</span></span><span style="display:flex;"><span>x &lt;- replicate(10000, rlkjcorr(1, 2, 1000)[1,2])
</span></span><span style="display:flex;"><span>hist(x, xlim = c(-1, 1)) <span style="color:#008000"># concentrated on 0, essentially no variability!</span>
</span></span></code></pre></div><p>This is cool, but inspecting hte rlkjcorr code, i see that for a 2x2 correlation matrix, the off-diagonal is just follows a beta, with the concentration parameter eta serving as both the beta parameters! In other words:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>y &lt;- 2 * rbeta(10000, 1000, 1000) - 1
</span></span><span style="display:flex;"><span>hist(y, xlim = c(-1, 1))
</span></span></code></pre></div><p>Produces identical output!</p>
<h1 id="6---survival-analysis">6 - survival analysis</h1>
<p><a href="https://ermeel86.github.io/case_studies/surv_stan_example.html">https://ermeel86.github.io/case_studies/surv_stan_example.html</a>
<a href="https://raw.githubusercontent.com/ermeel86/asurvivalmodelinstan/master/surv_stan_example.Rmd">https://raw.githubusercontent.com/ermeel86/asurvivalmodelinstan/master/surv_stan_example.Rmd</a></p>
<blockquote>
<p>Survival times in months after mastectomy of women with breast cancer. The cancers are classified as having metastized or not based on a histochemical marker.</p>
</blockquote>
<p><em>(Metastasis is a pathogenic agent&rsquo;s spread from an initial or primary site to a different or secondary site within the host&rsquo;s body)</em></p>
<p>B. S. Everitt and S. Rabe-Hesketh (2001), Analysing Medical Data using S-PLUS, Springer, New York, USA.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>library(HSAUR)
</span></span><span style="display:flex;"><span>data(mastectomy)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(HSAUR)
</span></span><span style="display:flex;"><span>library(tibble)
</span></span><span style="display:flex;"><span>library(dplyr)
</span></span><span style="display:flex;"><span>data(<span style="color:#a31515">&#34;mastectomy&#34;</span>)
</span></span><span style="display:flex;"><span>df &lt;- as_tibble(mastectomy)
</span></span><span style="display:flex;"><span>df &lt;- df %&gt;% mutate(metastized=as.double(metastized==<span style="color:#a31515">&#34;yes&#34;</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># time - integer, 5 to 225, months post-surgery</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># event - 26 TRUE, 18 FALSE, died during observation period</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># metastized - 32 yes, 12 no, whether the cancer metastized prior to surgery</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># crosstab:</span>
</span></span><span style="display:flex;"><span>table(d$metastized, d$event)
</span></span><span style="display:flex;"><span><span style="color:#008000">#     FALSE TRUE</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># no      7    5</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># yes    11   21</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># clearly huge increase in chance of death if metastized = &#34;yes&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_code &lt;- write_stan_file(<span style="color:#a31515">&#34;data {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower=1&gt; n_women;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower=0, upper=1&gt; censored[n_women];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  int&lt;lower=1, upper=2&gt; metastized[n_women];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real time[n_women];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  vector[2] a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  a ~ normal(5, 2);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  for (i in 1:n_women) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real log_mu = a[metastized[i]];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    if (censored[i] == 0) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">      target += exponential_lpdf(time[i] | exp(-log_mu));
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">      target += exponential_lccdf(time[i] | exp(-log_mu));
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">  real b_meta = a[2] - a[1];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model &lt;- cmdstan_model(model_code)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dl &lt;- list(
</span></span><span style="display:flex;"><span>  n_women = nrow(df),
</span></span><span style="display:flex;"><span>  time = df$time,
</span></span><span style="display:flex;"><span>  metastized = df$metastized + 1,
</span></span><span style="display:flex;"><span>  censored = as.numeric(!df$event)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit &lt;- model$sample(chains = 4, iter_warmup = 2000, iter_sampling = 4000, data = dl)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(posterior)
</span></span><span style="display:flex;"><span>post &lt;- as_draws_rvars(fit$draws())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dens(draws_of(post$b_meta))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean(draws_of(post$b_meta))
</span></span><span style="display:flex;"><span>HPDI(draws_of(post$b_meta))
</span></span></code></pre></div><p>We should be able to graph the two survival curves using a right?</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>apply(draws_of(post$a), 2, mean)
</span></span><span style="display:flex;"><span><span style="color:#008000"># 5.813379 4.871151</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>curve(1 - pexp(x, exp(-4.871151)), from = 0, to = 225)
</span></span><span style="display:flex;"><span>curve(1 - pexp(x, exp(-5.813379)), add = <span style="color:#00f">TRUE</span>)
</span></span></code></pre></div><p><a href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator">https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator</a></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># compare to survival package output</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(survival)
</span></span><span style="display:flex;"><span>fit_cox &lt;- coxph(Surv(time, event)~metastized, data=df)
</span></span><span style="display:flex;"><span>coef_cox &lt;- coef(fit_cox)
</span></span><span style="display:flex;"><span>se_cox &lt;- sqrt(fit_cox$var)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(survminer)
</span></span><span style="display:flex;"><span>fit_ &lt;- survfit(Surv(time, event)~metastized, data=mutate(df, metastized=as.logical(metastized)))
</span></span><span style="display:flex;"><span>dat_surv &lt;- ggsurvplot(fit_, data = df,conf.int = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>dat_surv
</span></span></code></pre></div><h1 id="7---using-stan-as-a-simulator">7 - using Stan as a simulator</h1>
<p>The <code>generated quantities</code> block can be used to produce stochastic data, akin to the <code>rnorm</code>, <code>rpois</code>, etc. functions in R. This is useful to double-check the behavior of the stochastic parameters.</p>
<p>Let&rsquo;s make a Stan program that emulates the behavior of <code>rnorm</code> in R. First, let&rsquo;s hard-code the mean and sd to be 0 and 1, the model is called <code>gaussian_no_data</code>. In that case, there&rsquo;s only one input: the number of samples to draw (<code>iter_sampling</code>).</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_no_data$sample(iter_sampling = 10, fixed_param = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y&#34;</span>]
</span></span></code></pre></div><p>We need <code>fixed_param</code> because there is nothing in the stan <code>parameters</code> or <code>model</code> block (so nothing to run the sampler on), so the sampler diagnostics would otherwise fail. Also, if <code>fixed_param = TRUE</code>, chains is set to 1.</p>
<p>We can control the behavior a bit more with parameters entered into the <code>data</code> block.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars &lt;- list(
</span></span><span style="display:flex;"><span>  N = 100,
</span></span><span style="display:flex;"><span>  m = 10,
</span></span><span style="display:flex;"><span>  s = 1
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_m_s$sample(iter_sampling = pars$N, fixed_param = <span style="color:#00f">TRUE</span>, data = pars)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y&#34;</span>]
</span></span></code></pre></div><p>This is <code>rnorm</code> in its full functionality now.</p>
<p>We can define as many quantities in our <code>generated quantities</code> as we wish. In <code>gaussian_two_outcome</code>, we create reals <code>y</code> and <code>z</code> which are independent of each other. The function then is</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars &lt;- list(
</span></span><span style="display:flex;"><span>  N = 100
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_two_outcome$sample(iter_sampling = pars$N, fixed_param = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y&#34;</span>]
</span></span><span style="display:flex;"><span>z &lt;- post[,<span style="color:#a31515">&#34;z&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_two_outcome_ii$sample(iter_sampling = pars$N, fixed_param = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y[1]&#34;</span>]
</span></span><span style="display:flex;"><span>z &lt;- post[,<span style="color:#a31515">&#34;y[2]&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_two_outcome_iii$sample(iter_sampling = pars$N, fixed_param = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y[1]&#34;</span>]
</span></span><span style="display:flex;"><span>z &lt;- post[,<span style="color:#a31515">&#34;y[2]&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_two_outcome_vec$sample(iter_sampling = pars$N, fixed_param = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y[1]&#34;</span>]
</span></span><span style="display:flex;"><span>z &lt;- post[,<span style="color:#a31515">&#34;y[2]&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stanout &lt;- models$gaussian_two_outcome_vec_rho$sample(iter_sampling = pars$N, fixed_param = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>post_array &lt;- stanout$draws() <span style="color:#008000"># iterations x chains x variable</span>
</span></span><span style="display:flex;"><span>post &lt;- as.data.frame(as_draws_df(post_array)) <span style="color:#008000"># (iterations x chains) x variable</span>
</span></span><span style="display:flex;"><span>y &lt;- post[,<span style="color:#a31515">&#34;y[1]&#34;</span>]
</span></span><span style="display:flex;"><span>z &lt;- post[,<span style="color:#a31515">&#34;y[2]&#34;</span>]
</span></span><span style="display:flex;"><span>cor(y, z)
</span></span></code></pre></div><p>Note: if you iterate only once, it doesn&rsquo;t return proper output, which sucks.</p>
<p>Note you can&rsquo;t simply have a stan program that returns a constant (I gues?).</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">
generated quantities {
  real y[2] = {1, 100};
}
</code></pre><p>This also doesn&rsquo;t work:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">
generated quantities {
  vector[2] y = [1, 100]&#39;;
}
</code></pre><p>Simulating this way is useful to understand a bit of how Stan organizes things. Normally the fixed values we add to our simulations are called &ldquo;parameters&rdquo;, but here those would go into the <code>data</code> block. Meanwhile, what Stan calls <code>parameters</code> are exclusively <em>stochastic</em> quantities, which need Stan-defined probability distributions to participate in sampling and update the <code>target</code>. The <code>data</code> block (aka the input parameters) are distinct.</p>
<p>How could I use this in my own work? If I feed in a particular person for CAC, they have a CAC score, and also age and sex and ethnicity. The choice of priors, then, would inform a y_sim value for that person which describes the prior predictive density. If that&rsquo;s not unreasonable, we have a good model! So, it makes sense to check, I guess. But I don&rsquo;t think I&rsquo;d be able to create a time series in this thing&hellip;or could I?</p>
<p>It also helps us understand that stochastically-generated quantities are the <em>only</em> thing output by Stan, and they all have N iterations.</p>
<p><a href="https://khakieconomics.github.io/2017/04/30/An-easy-way-to-simulate-fake-data-in-stan.html">Jim Savage recommends</a> incorporating simulation code into <code>generated quantities</code> to confirm the realism of prior specifications. This involves &ldquo;switching off&rdquo; the updating.</p>
<p>The cmdstanr flag <code>fixed_param</code> may also do this, I need to test.</p>
<h1 id="8---validating-models">8 - validating models</h1>
<p>These notes are mostly drawing on Chapter 29 in the <code>stan-users-guide-2_28.pdf</code> and BDA3 Chapter 7. Also see here for cmdstanr&rsquo;s loo: <a href="https://mc-stan.org/cmdstanr/reference/fit-method-loo.html">https://mc-stan.org/cmdstanr/reference/fit-method-loo.html</a></p>
<p>The gold standard for statistical models is <em>predictive</em> accuracy. There are two common performance measures for comparison of the predictions for two models: squared error, and surprise (a.k.a. negative log probability). A model that has lower squared-error, or a model that has lower surprise, is considered better for that prediction.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y &lt;- 1.2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>-dnorm(y, 0, 1, log = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>-dnorm(y, 1.2, 1, log = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>-dnorm(y, 1.2, 1e-25, log = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>-dnorm(y, 1.2, 1e-100, log = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>-dnorm(y, 1.2, 0, log = <span style="color:#00f">TRUE</span>) <span style="color:#008000"># negative infinity! lowest surprise possible</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- rnorm(10000, 0, 1)
</span></span><span style="display:flex;"><span>mean((y - m0)^2) <span style="color:#008000"># some error</span>
</span></span><span style="display:flex;"><span>m0 &lt;- rnorm(10000, 1.2, 1)
</span></span><span style="display:flex;"><span>mean((y - m0)^2) <span style="color:#008000"># smaller error</span>
</span></span><span style="display:flex;"><span>m0 &lt;- rnorm(10000, 1.2, 1e-100)
</span></span><span style="display:flex;"><span>mean((y - m0)^2) <span style="color:#008000"># effectively no error!</span>
</span></span></code></pre></div><p>Info criteria like AIC, WAIC, DIC, BIC, PSIS, and LOO are fundamentally all approximations of this validation performance, made in the case where we DONT have out-of-sample validation metrics. The method of cross-validation is a generalization of LOO (or, rather, LOO is an extreme version of cross-validation), again in the context of not having an out-of-sample validation system.</p>
<h2 id="a-note-about--and-probability-functions">a note about += and probability functions</h2>
<p>In Stan, log-probability functions that take vector inputs automatically += the output. That is,</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">real log_p1 = 0
for (i in 1:N) log_p1 += normal_lpdf(y[i] | mu, sigma);
// same thing as
real log_p2 = normal_lpdf(y | mu, sigma);
</code></pre><p>This is why we can write target statements in this way:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">for (i in 1:N) target += normal_lpdf(y[i] | mu, sigma);
// same thing as
target += normal_lpdf(y | mu, sigma);
</code></pre><p>To perform a cross-validation with N sets, we need to fit the models N times to the data, each time extracting the <em>predicted</em> performance for the validation set, and store <em>all</em> validation set predictions together.</p>
<h2 id="comparing-three-models">comparing three models</h2>
<p>Here we simulate a hierarchical Gaussian regression model with categorical groups and a continuous predictor, and compare three statistical models fit to the observed data — a simple Gaussian outcome model, a &lsquo;complete pooling&rsquo; model that doesn&rsquo;t know about the groups, and a hierarchical model which looks exactly like the data generating process.</p>
<p>I can program generated quantities to work as cross-validation as follows.</p>
<p>For any given model run, I can create a parallel model in the generated quantites with new observations.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat &lt;- list(
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># sim parameters</span>
</span></span><span style="display:flex;"><span>  N_obs = 50,
</span></span><span style="display:flex;"><span>  N_grp = 20,
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># prior parameters</span>
</span></span><span style="display:flex;"><span>  a_pop_mu = 0,
</span></span><span style="display:flex;"><span>  a_pop_sigma = 1,
</span></span><span style="display:flex;"><span>  a_grp_sigma_rate = 1,
</span></span><span style="display:flex;"><span>  b_pop_mu = 1,
</span></span><span style="display:flex;"><span>  b_pop_sigma = 1,
</span></span><span style="display:flex;"><span>  b_grp_sigma_rate = 1,
</span></span><span style="display:flex;"><span>  sigma_rate = 1
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat$id &lt;- sample(seq_len(dat$N_grp), dat$N_obs, replace = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars &lt;- list(
</span></span><span style="display:flex;"><span>  a_grp_sigma = rexp(1, dat$a_grp_sigma_rate),
</span></span><span style="display:flex;"><span>  b_grp_sigma = rexp(1, dat$b_grp_sigma_rate),
</span></span><span style="display:flex;"><span>  a_pop = rnorm(1, dat$a_pop_mu, dat$a_pop_sigma),
</span></span><span style="display:flex;"><span>  b_pop = rnorm(1, dat$b_pop_mu, dat$b_pop_sigma),
</span></span><span style="display:flex;"><span>  a_grp = rnorm(dat$N_grp, 0, dat$a_grp_sigma),
</span></span><span style="display:flex;"><span>  b_grp = rnorm(dat$N_grp, 0, dat$b_grp_sigma),
</span></span><span style="display:flex;"><span>  sigma = rexp(1, dat$sigma_rate)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pars$a &lt;- pars$a_pop + pars$a_grp * pars$a_grp_sigma
</span></span><span style="display:flex;"><span>pars$b &lt;- pars$b_pop + pars$b_grp * pars$b_grp_sigma
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat$x &lt;- rnorm(dat$N_obs, 0, 1)
</span></span><span style="display:flex;"><span>mu &lt;- pars$a[dat$id] + pars$b[dat$id] * dat$x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat$y &lt;- rnorm(dat$N_obs, mu, pars$sigma)
</span></span></code></pre></div><p>The standard approach is to fit some models and calculate comparison statistics.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat$in_validation &lt;- rep(<span style="color:#00f">FALSE</span>, dat$N_obs)
</span></span><span style="display:flex;"><span>dat$in_training &lt;- rep(<span style="color:#00f">TRUE</span>, dat$N_obs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stan_data &lt;- list(
</span></span><span style="display:flex;"><span>  M = dat$N_grp,
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># training data</span>
</span></span><span style="display:flex;"><span>  N = sum(dat$in_training),
</span></span><span style="display:flex;"><span>  x = dat$x[dat$in_training],
</span></span><span style="display:flex;"><span>  id = dat$id[dat$in_training],
</span></span><span style="display:flex;"><span>  y = dat$y[dat$in_training],
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># validation data</span>
</span></span><span style="display:flex;"><span>  N_v = sum(dat$in_validation),
</span></span><span style="display:flex;"><span>  x_v = dat$x[dat$in_validation],
</span></span><span style="display:flex;"><span>  id_v = dat$id[dat$in_validation],
</span></span><span style="display:flex;"><span>  y_v = dat$y[dat$in_validation],
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># prior parameters</span>
</span></span><span style="display:flex;"><span>  a_pop_mu = dat$a_pop_mu,
</span></span><span style="display:flex;"><span>  a_pop_sigma = dat$a_pop_sigma,
</span></span><span style="display:flex;"><span>  a_ind_sigma_rate = dat$a_grp_sigma_rate,
</span></span><span style="display:flex;"><span>  b_pop_mu = dat$b_pop_mu,
</span></span><span style="display:flex;"><span>  b_pop_sigma = dat$b_pop_sigma,
</span></span><span style="display:flex;"><span>  b_ind_sigma_rate = dat$b_grp_sigma_rate,
</span></span><span style="display:flex;"><span>  sigma_rate = dat$sigma_rate
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_gaussian_test &lt;- models$gaussian_cv$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 500
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_regression_test &lt;- models$regression_cv$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 500
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_hierarchical_test &lt;- models$multilevel_regression_noncentered_cv$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>  iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>  max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>  refresh = 500
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># the bigger the eloo, the better the model</span>
</span></span><span style="display:flex;"><span>fit_gaussian_test$loo() <span style="color:#008000"># -125</span>
</span></span><span style="display:flex;"><span>fit_regression_test$loo() <span style="color:#008000"># -115</span>
</span></span><span style="display:flex;"><span>fit_hierarchical_test$loo() <span style="color:#008000"># 9</span>
</span></span></code></pre></div><p>LOO statistics indicate that the heirarchical model is the best approximation of the DGP. But we can also score this using cross-validation:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sample_safe &lt;- function(x, <span style="color:#00f">...</span>) {
</span></span><span style="display:flex;"><span>  x[sample.int(length(x), <span style="color:#00f">...</span>)]
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>assign_sets &lt;- function(n, n_sets) {
</span></span><span style="display:flex;"><span>  if (n &lt; n_sets) stop(<span style="color:#a31515">&#34;there must be more individuals than bins!&#34;</span>)
</span></span><span style="display:flex;"><span>  i &lt;- 1:n
</span></span><span style="display:flex;"><span>  n_per_set &lt;- rep(floor(n/n_sets), each = n_sets)
</span></span><span style="display:flex;"><span>  n_xtra &lt;- n %% n_sets
</span></span><span style="display:flex;"><span>  if (n_xtra &gt; 0) {
</span></span><span style="display:flex;"><span>    for (i in 1:n_xtra) {
</span></span><span style="display:flex;"><span>      add_index &lt;- sample(1:n_sets, 1, replace = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>      n_per_set[add_index] &lt;- n_per_set[add_index] + 1
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  set &lt;- rep(1:n_sets, times = n_per_set)
</span></span><span style="display:flex;"><span>  set &lt;- sample_safe(set)
</span></span><span style="display:flex;"><span>  return(set)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_cv_sets &lt;- 10
</span></span><span style="display:flex;"><span>dat$cv_set &lt;- assign_sets(dat$N_obs, n_cv_sets)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>gaussian_lppd &lt;- 0
</span></span><span style="display:flex;"><span>regression_lppd &lt;- 0
</span></span><span style="display:flex;"><span>hierarchical_lppd &lt;- 0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># set the first set as the validation set</span>
</span></span><span style="display:flex;"><span>for (i in 1:n_cv_sets) {
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  dat$in_validation &lt;- dat$cv_set == i
</span></span><span style="display:flex;"><span>  dat$in_training &lt;- !dat$in_validation
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  stan_data &lt;- list(
</span></span><span style="display:flex;"><span>    M = dat$N_grp,
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># training data</span>
</span></span><span style="display:flex;"><span>    N = sum(dat$in_training),
</span></span><span style="display:flex;"><span>    x = dat$x[dat$in_training],
</span></span><span style="display:flex;"><span>    id = dat$id[dat$in_training],
</span></span><span style="display:flex;"><span>    y = dat$y[dat$in_training],
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># validation data</span>
</span></span><span style="display:flex;"><span>    N_v = sum(dat$in_validation),
</span></span><span style="display:flex;"><span>    x_v = dat$x[dat$in_validation],
</span></span><span style="display:flex;"><span>    id_v = dat$id[dat$in_validation],
</span></span><span style="display:flex;"><span>    y_v = dat$y[dat$in_validation],
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># prior parameters</span>
</span></span><span style="display:flex;"><span>    a_pop_mu = dat$a_pop_mu,
</span></span><span style="display:flex;"><span>    a_pop_sigma = dat$a_pop_sigma,
</span></span><span style="display:flex;"><span>    a_ind_sigma_rate = dat$a_grp_sigma_rate,
</span></span><span style="display:flex;"><span>    b_pop_mu = dat$b_pop_mu,
</span></span><span style="display:flex;"><span>    b_pop_sigma = dat$b_pop_sigma,
</span></span><span style="display:flex;"><span>    b_ind_sigma_rate = dat$b_grp_sigma_rate,
</span></span><span style="display:flex;"><span>    sigma_rate = dat$sigma_rate
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  fit_gaussian_test &lt;- models$gaussian_cv$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>    iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>    max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>    refresh = 500
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  fit_regression_test &lt;- models$regression_cv$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>    iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>    max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>    refresh = 500
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  fit_hierarchical_test &lt;- models$multilevel_regression_noncentered_cv$sample(parallel_chains = 3, chains = 3,
</span></span><span style="display:flex;"><span>    iter_warmup = 1500, iter_sampling = 3000, adapt_delta = 0.8,
</span></span><span style="display:flex;"><span>    max_treedepth = 10, data = stan_data, step_size = 0.1, seed = 1,
</span></span><span style="display:flex;"><span>    refresh = 500
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  library(posterior)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#008000"># stash the likelihoods for each fold</span>
</span></span><span style="display:flex;"><span>  samples &lt;- as.data.frame(as_draws_df(fit_gaussian_test$draws()))
</span></span><span style="display:flex;"><span>  gaussian_lppd &lt;- gaussian_lppd + log_sum_exp(samples$log_p) - log(length(samples$log_p))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  samples &lt;- as.data.frame(as_draws_df(fit_regression_test$draws()))
</span></span><span style="display:flex;"><span>  regression_lppd &lt;- regression_lppd + log_sum_exp(samples$log_p) - log(length(samples$log_p))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  samples &lt;- as.data.frame(as_draws_df(fit_hierarchical_test$draws()))
</span></span><span style="display:flex;"><span>  hierarchical_lppd &lt;- hierarchical_lppd + log_sum_exp(samples$log_p) - log(length(samples$log_p))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  print(paste(<span style="color:#a31515">&#34;fit validation set&#34;</span>, i))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>gaussian_lppd <span style="color:#008000"># -125</span>
</span></span><span style="display:flex;"><span>regression_lppd <span style="color:#008000"># -116</span>
</span></span><span style="display:flex;"><span>hierarchical_lppd <span style="color:#008000"># 2.06</span>
</span></span></code></pre></div><p>As expected, the generating model has the highest log-probability (lowest surprise). The LOO statistics are <em>very</em> close to the 10-fold cross validation too!</p>
<h1 id="9---noisy-categorization">9 - noisy categorization</h1>
<p><em>adapted from section 15.4 of STAN manual, after Dawid and Skene (1979)</em></p>
<p>In this model, we have $I$ items to classify into one of $K$ possible categories. There is some underlying &ldquo;true&rdquo; category each item belongs in, and the rater is not perfect.</p>
<p>Our goal is to estimate the assigment probabilities by the rater, given an item of any particular &ldquo;true&rdquo; category. Note that is a very different exercise than simply describing the unconditional assignment probabilities to each category, because it conditions on the true underlying categories.</p>
<p>This gives us the remarkable ability to estimate the mis-categorization rates <em>even though we do not know the true categories</em>, so long as we have some basic estimates of their distribution and prior information on the conditional misclassification rates.</p>
<p>Could allow us to ask, how diverse is any one category? How quickly can true category be identified by some kind of multirater consensus? How often do we expect errors, etc?</p>
<p>(I don&rsquo;t quite understand how these parameters are updated by the observations, but someday I will.)</p>
<p>For input data, we have the rater-assignments table. We also include as data $K$-vector $\alpha$ which represents our prior knowledge of the relative abundances of each of the categories, and the matrix $\beta$, which provides our prior information on the assignment distribtion of each rater, conditional on the item category and for each rater.</p>
<p>The Manual states that &ldquo;reasonable weakly informative priors&rdquo; as alpha_k = 3 for each category k and beta_k,k = 2.5 * K for k=k and beta_k,k&rsquo; for k != k&rsquo;&rdquo;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_items &lt;- 20
</span></span><span style="display:flex;"><span>n_categories &lt;- 3
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ratings &lt;- rep(<span style="color:#00f">NA</span>, length = n_items)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>for(i in 1:n_items){
</span></span><span style="display:flex;"><span>  ratings[i] &lt;- sample(1:n_categories, 1)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>alpha &lt;- as.vector(softmax(rep(1, n_categories)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>beta &lt;- list()
</span></span><span style="display:flex;"><span>for(j in 1:n_categories) beta[[j]] &lt;- alpha
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  I = n_items,
</span></span><span style="display:flex;"><span>  K = n_categories,
</span></span><span style="display:flex;"><span>  y = ratings,
</span></span><span style="display:flex;"><span>  alpha = alpha,
</span></span><span style="display:flex;"><span>  beta =  beta
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0_bin &lt;- stan_model(file = <span style="color:#a31515">&#34;../stan/noisy-categorization-simple.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- sampling(model0_bin, data = dat_list)
</span></span></code></pre></div><p>The Manual presents this is a more general form, imaging that each item will be classified multiple times, once by each of $J$ raters.</p>
<p>Now we parameterize each rater $j$&rsquo;s assignment probability to category $k$ by a vector <code>theta[j, k]</code> which is itself a $K$-simplex, since you need one such schedule for each &ldquo;true&rdquo; category of item.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_items &lt;- 20
</span></span><span style="display:flex;"><span>n_raters &lt;- 10
</span></span><span style="display:flex;"><span>n_categories &lt;- 3
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ratings &lt;- matrix(<span style="color:#00f">NA</span>, nrow = n_items, ncol = n_raters)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>for(i in 1:n_items){
</span></span><span style="display:flex;"><span>  for(j in 1:n_raters){
</span></span><span style="display:flex;"><span>    ratings[i, j] &lt;- sample(1:n_categories, 1)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>alpha &lt;- as.vector(softmax(rep(1, n_categories)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>beta &lt;- list()
</span></span><span style="display:flex;"><span>for(j in 1:n_categories) beta[[j]] &lt;- alpha
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  I = n_items,
</span></span><span style="display:flex;"><span>  J = n_raters,
</span></span><span style="display:flex;"><span>  K = n_categories,
</span></span><span style="display:flex;"><span>  y = ratings,
</span></span><span style="display:flex;"><span>  alpha = alpha,
</span></span><span style="display:flex;"><span>  beta =  beta
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0_bin &lt;- stan_model(file = <span style="color:#a31515">&#34;../stan/noisy-categorization.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- sampling(model0_bin, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m0)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dim(post$theta)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dim(post$theta[,1,,])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>apply(post$theta[,1,1,], 2, mean)
</span></span></code></pre></div><p>For each of the J raters, theta gives a K x K of simplex for how they classify things for each of the true categories.</p>
<p>Given that one is a &ldquo;true&rdquo; value, we might ask what proportion of probability mass is concentraed on that true value for each rater for each category, and the aggregation of that reflects how accurate the raters are.</p>
<p>We can calculate on overall mis-classification rate and rater-specific rates in the same way.</p>
<p>The Manual mentions that a next step would be &ldquo;to extend the model to include a hierarchical prior for $\beta$ and to partially pool the estimates of coder accuracy and bias.&rdquo; I should try this!</p>
<p>Some things I learned about STAN from this exercise:</p>
<ul>
<li>
<p>nothing wrong with including <code>print</code> statements within the model, so you can visually inspect what the contents of different objects actually are</p>
</li>
<li>
<p>models report both parameters and transformed parameters</p>
</li>
</ul>
<h1 id="10---item-response-theory">10 - item response theory</h1>
<p>(Bafumi et al. 2005; Fox 2010; Jackman 2001; Schacht and Grote 2015)</p>
<p>Say you had a continuous variable like &ldquo;height&rdquo; that cannot be observed, but is highly correlated with the answers to binary questions. So you might ask, &ldquo;are you uncomfortable sitting in airline seats?&rdquo;, &ldquo;are you good at basketball?&rdquo;, &ldquo;do you have difficulty finding clothes that fit you?&rdquo;</p>
<p>The answers will not be perfectly correlated with height, but the strength of the correlation will vary; &ldquo;yes&rdquo; and &ldquo;no&rdquo; are equally likely on some across the x, but on others, only people at the extreme ends of x will say yes or no. For others, it will be about halfway through the distribution that people say &ldquo;yes&rdquo;, good bisecting questions</p>
<p>The model for each of these questions, when we know the outcome, will be</p>
<pre tabindex="0"><code>  pr(yes) = logistic( a_i + b_i * x)
</code></pre><p>q1 is generally true of taller people, a_i is 0 and b_i is high</p>
<p>q2 is only slightly true of taller peopel, a_i is 0 but b_i is small</p>
<p>q3 is true of the very tall and quite sharp, a_i is negative and b_i is very high</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  n &lt;- 30
</span></span><span style="display:flex;"><span>  x &lt;- rnorm(n, 0, 1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  logistic &lt;- function(x) exp(x)/(1+exp(x))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  pr_1 &lt;- logistic( 0 + 2 * x )
</span></span><span style="display:flex;"><span>  pr_2 &lt;- logistic( 0 + 0.1 * x )
</span></span><span style="display:flex;"><span>  pr_3 &lt;- logistic( -2 + 2 * x ) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  curve(logistic(2*x), from=-3, to=3)
</span></span><span style="display:flex;"><span>  curve(logistic(0.1*x), from=-3, to=3, add=<span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>  curve(logistic(-2 + 2*x), from=-3, to=3, add=<span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  curve(dnorm(x), add=<span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  q1 &lt;- rbinom(n, 1, prob=pr_1)
</span></span><span style="display:flex;"><span>  q2 &lt;- rbinom(n, 1, prob=pr_2)
</span></span><span style="display:flex;"><span>  q3 &lt;- rbinom(n, 1, prob=pr_3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  tapply(x, q1, mean)
</span></span><span style="display:flex;"><span>  tapply(x, q2, mean)
</span></span><span style="display:flex;"><span>  tapply(x, q3, mean)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  beta1 &lt;- 0    <span style="color:#008000"># -a_i/b_i</span>
</span></span><span style="display:flex;"><span>  beta2 &lt;- 0    <span style="color:#008000"># -a_i/b_i</span>
</span></span><span style="display:flex;"><span>  beta3 &lt;- (1)  <span style="color:#008000"># -a_i/b_i</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  gamma1 &lt;- 1/2    <span style="color:#008000"># b_i / 4</span>
</span></span><span style="display:flex;"><span>  gamma2 &lt;- 0.025  <span style="color:#008000"># b_i / 4</span>
</span></span><span style="display:flex;"><span>  gamma3 &lt;- 1/2    <span style="color:#008000"># b_i / 4 </span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  wide &lt;- data.frame(x, q1, q2, q3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  response &lt;- c(q1, q2, q3)
</span></span><span style="display:flex;"><span>  item &lt;- rep(c(1,2,3), each=length(q1))
</span></span><span style="display:flex;"><span>  person &lt;- rep(1:n, times=3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  long &lt;- data.frame(item, response, person)
</span></span></code></pre></div><p>where a_i and b_i are specific to question i, and x is the latent variable.</p>
<p>The challenge is when we do not have a way to directly measure the x.</p>
<p>Define gamma_i and beta_i for each instrument i. We can use these to describe the shape and slope of the function, even though we do not have the x itself. Can we estimate these though?</p>
<p>John: the IRT model assumes you have different individuals, you have to run it with a_k for each individual k..</p>
<p>my challenge: if you have two variables with 50% yes and no, any gamma is consistent with that, assuming an underlying normal distribution
&hellip;and what if the distribution isn&rsquo;t normal at all, but quite skewed, such that 80% are no? then an 80% no question isn&rsquo;t going to work&hellip;.</p>
<p>the only way to test a latent variable model is to write out its assumptions</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  library(rethinking)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  irt_model &lt;- alist(
</span></span><span style="display:flex;"><span>    item ~ dbinom(1, p),
</span></span><span style="display:flex;"><span>    logit(p) &lt;- gamma[item] * (alpha[person] - beta[item]),
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># gamma and beta mean what they mean above</span>
</span></span><span style="display:flex;"><span>    gamma[item] ~ dnorm(0, 2),
</span></span><span style="display:flex;"><span>    alpha[person] ~ dnorm(0, 2),
</span></span><span style="display:flex;"><span>    beta[item] ~ dexp(2)
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  wide$item &lt;- wide$q1
</span></span><span style="display:flex;"><span>  m1 &lt;- map2stan(irt_model, wide)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  wide$item &lt;- wide$q2
</span></span><span style="display:flex;"><span>  m2 &lt;- map2stan(irt_model, wide)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  wide$item &lt;- wide$q3
</span></span><span style="display:flex;"><span>  m3 &lt;- map2stan(irt_model, wide)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  library(rethinking)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  irt_model &lt;- alist(
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    response ~ dbinom(1, p),
</span></span><span style="display:flex;"><span>    logit(p) &lt;- gamma[item] * (alpha[person] - beta[item]),
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># gamma and beta mean what they mean above</span>
</span></span><span style="display:flex;"><span>    gamma[item] ~ dnorm(0, 2),
</span></span><span style="display:flex;"><span>    alpha[person] ~ dnorm(0, 2),
</span></span><span style="display:flex;"><span>    beta[item] ~ dexp(2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  m0 &lt;- map2stan(irt_model, long)
</span></span></code></pre></div><p>How do you get something from nothing?</p>
<p>The alpha[person] is an estimate, approx, of underlying latent axis.</p>
<p>post &lt;- extract.samples(m0)</p>
<p>Another way to think about it, is the alpha[person] is an approximation of the latent variables, you are basically urnning the model as an imputation model with all your latent values missing.</p>
<p>But there&rsquo;s a few things that are interesting / worrysom&hellip;doesn&rsquo;t this depend critically on the prior distribution of the latent variable?</p>
<h1 id="11---markov-models">11 - markov models</h1>
<p><em>adapted from section 15.4 of STAN manual, after Dawid and Skene (1979)</em></p>
<h2 id="a-simple-markov-model">a simple Markov model</h2>
<p>Here&rsquo;s how we&rsquo;d estimate transition probabilities in a markov chain.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>softmax &lt;- function(x){
</span></span><span style="display:flex;"><span>  exp(x) / sum(exp(x))
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_states &lt;- 5
</span></span><span style="display:flex;"><span>n &lt;- 10000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain &lt;- rep(<span style="color:#00f">NA</span>, length = n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>alpha &lt;- as.vector(softmax(rep(1, n_states)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># define a transition matrix, which we want to estimate</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M &lt;- matrix(runif(n_states^2), nrow = n_states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M &lt;- t(apply(M, 1, softmax))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># initialize chains</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chain[1] &lt;- sample(1:n_states, 1, prob = alpha)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>for(i in 2:n){
</span></span><span style="display:flex;"><span>  chain[i] &lt;- sample(1:n_states, 1, prob = M[chain[i-1],])
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  K = n_states,
</span></span><span style="display:flex;"><span>  T = n,
</span></span><span style="display:flex;"><span>  z = chain,
</span></span><span style="display:flex;"><span>  alpha = alpha
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0_bin &lt;- stan_model(file = <span style="color:#a31515">&#34;../stan/basic-markov-model.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- sampling(model0_bin, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m0)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post$theta[,1,]
</span></span></code></pre></div><p>There&rsquo;s sometign missing tho, which is a prior on the transition states. Can we include?</p>
<p>Yes, but I found out that dirichlet doesn&rsquo;t like input vector with 0s, so I tried very small numbers instead.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>theta_prior &lt;- matrix(.01, nrow = n_states, ncol = n_states)
</span></span><span style="display:flex;"><span>diag(theta_prior) &lt;- .99
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  K = n_states,
</span></span><span style="display:flex;"><span>  T = n,
</span></span><span style="display:flex;"><span>  z = chain,
</span></span><span style="display:flex;"><span>  theta_prior = theta_prior
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0_bin &lt;- stan_model(file = <span style="color:#a31515">&#34;../stan/markov-model-transition-prior.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- sampling(model0_bin, data = dat_list)
</span></span></code></pre></div><p>two-steps-back dependencies is what the tetris algorithm is!</p>
<p>Doing it this way is tremendously inefficient though, since the majority of transition probabilities are 0 a prior. Better to have multiple transition matricies&hellip;
Then pass in a vector</p>
<p>doing it another way is tremendously ineffiicient&hellip;</p>
<h2 id="a-supervised-markov-model">a supervised Markov model</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>softmax &lt;- function(x){
</span></span><span style="display:flex;"><span>  exp(x) / sum(exp(x))
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_latent_states &lt;- 5
</span></span><span style="display:flex;"><span>n_output_states &lt;- 2
</span></span><span style="display:flex;"><span>n &lt;- 10000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>latent_chain &lt;- rep(<span style="color:#00f">NA</span>, length = n)
</span></span><span style="display:flex;"><span>output_chain &lt;- rep(<span style="color:#00f">NA</span>, length = n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>alpha &lt;- as.vector(softmax(rep(1, n_latent_states)))
</span></span><span style="display:flex;"><span>beta &lt;- as.vector(softmax(rep(1, n_output_states)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># define a transition matrix, which we want to estimate</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M &lt;- matrix(runif(n_latent_states^2), nrow = n_latent_states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M &lt;- t(apply(M, 1, softmax))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>O &lt;- matrix(runif(n_latent_states * n_output_states), nrow = n_latent_states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>O &lt;- t(apply(O, 1, softmax))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># initialize chains</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>latent_chain[1] &lt;- sample(1:n_latent_states, 1, prob = alpha)
</span></span><span style="display:flex;"><span>output_chain[1] &lt;- sample(1:n_output_states, 1, prob = O[latent_chain[1],])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>for(i in 2:n){
</span></span><span style="display:flex;"><span>  latent_chain[i] &lt;- sample(1:n_latent_states, 1, prob = M[latent_chain[i-1],])
</span></span><span style="display:flex;"><span>  output_chain[i] &lt;- sample(1:n_output_states, 1, prob = O[latent_chain[i],])
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  K = n_latent_states,
</span></span><span style="display:flex;"><span>  V = n_output_states,
</span></span><span style="display:flex;"><span>  T = n,
</span></span><span style="display:flex;"><span>  z = latent_chain,
</span></span><span style="display:flex;"><span>  y = output_chain,
</span></span><span style="display:flex;"><span>  alpha = alpha,
</span></span><span style="display:flex;"><span>  beta = beta
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0_bin &lt;- stan_model(file = <span style="color:#a31515">&#34;../stan/supervised-markov-model.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- sampling(model0_bin, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m0)
</span></span></code></pre></div><p>This is a &ldquo;supervised&rdquo; model, we can see the states. I dunno what an unsupervised model would involve, probably speculating about the states given the output.</p>
<h2 id="a-hidden-markov-model">a hidden Markov model</h2>
<p>Now, instead of a supervised model, how to do a hidden model? We&rsquo;d have to marginalize over the unknown markov chain, I suppose&hellip;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>softmax &lt;- function(x){
</span></span><span style="display:flex;"><span>  exp(x) / sum(exp(x))
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_latent_states &lt;- 5
</span></span><span style="display:flex;"><span>n_output_states &lt;- 2
</span></span><span style="display:flex;"><span>n &lt;- 10000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>latent_chain &lt;- rep(<span style="color:#00f">NA</span>, length = n)
</span></span><span style="display:flex;"><span>output_chain &lt;- rep(<span style="color:#00f">NA</span>, length = n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>alpha &lt;- as.vector(softmax(rep(1, n_latent_states)))
</span></span><span style="display:flex;"><span>beta &lt;- as.vector(softmax(rep(1, n_output_states)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># define a transition matrix, which we want to estimate</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M &lt;- matrix(runif(n_latent_states^2), nrow = n_latent_states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>M &lt;- t(apply(M, 1, softmax))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>O &lt;- matrix(runif(n_latent_states * n_output_states), nrow = n_latent_states)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>O &lt;- t(apply(O, 1, softmax))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># initialize chains</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>latent_chain[1] &lt;- sample(1:n_latent_states, 1, prob = alpha)
</span></span><span style="display:flex;"><span>output_chain[1] &lt;- sample(1:n_output_states, 1, prob = O[latent_chain[1],])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>for(i in 2:n){
</span></span><span style="display:flex;"><span>  latent_chain[i] &lt;- sample(1:n_latent_states, 1, prob = M[latent_chain[i-1],])
</span></span><span style="display:flex;"><span>  output_chain[i] &lt;- sample(1:n_output_states, 1, prob = O[latent_chain[i],])
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dat_list &lt;- list(
</span></span><span style="display:flex;"><span>  K = n_latent_states,
</span></span><span style="display:flex;"><span>  V = n_output_states,
</span></span><span style="display:flex;"><span>  T = n,
</span></span><span style="display:flex;"><span>  y = output_chain,
</span></span><span style="display:flex;"><span>  alpha = alpha,
</span></span><span style="display:flex;"><span>  beta = beta
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0_bin &lt;- stan_model(file = <span style="color:#a31515">&#34;../stan/hidden-markov-model.stan&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m0 &lt;- sampling(model0_bin, data = dat_list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m0)
</span></span></code></pre></div><h2 id="second-order-models">second-order models</h2>
<p>for my tetris analysis I wrote a second-order model</p>
<h1 id="12---time-series-analysis">12 - time series analysis</h1>
<p>How do you incorporate time series?</p>
<h1 id="13---systems-of-partial-differential-equations">13 - systems of partial differential equations</h1>
<p>how do you incorporate PDE? there&rsquo;s the lynx and hare system&hellip;</p>
<h1 id="14---marginalization">14 - marginalization</h1>
<h2 id="defining-the-problem">defining the problem</h2>
<p>We start with a simple bivariate regression, with a continuous outcome $y$ modeled using a binary predictor $x$ via $y \sim N(\mu, \sigma)$ and $\mu = a + bx$, for some parameters $a$, $b$ and $\sigma$. When $x$ is 0, $\mu=a$, and when $x$ is 1, $\mu=a+b$, so $b$ is merely the marginal difference in mean outcome across the two levels of $x$.</p>
<p>First, we simulate some data with the following script:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>set.seed(1981)
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>x_baserate &lt;- 0.5
</span></span><span style="display:flex;"><span>a &lt;- 0
</span></span><span style="display:flex;"><span>b &lt;- 1
</span></span><span style="display:flex;"><span>x &lt;- sample( 0:1 , size=N , replace=<span style="color:#00f">TRUE</span> , prob=c(1-x_baserate,x_baserate) )
</span></span><span style="display:flex;"><span>y &lt;- rnorm( N , a + b*x , 1 )
</span></span><span style="display:flex;"><span>d &lt;- data.frame(y, x, stringsAsFactors=<span style="color:#00f">FALSE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot(jitter(d$x, amount=0.1), d$y)
</span></span></code></pre></div><p>Hopefully obvious how parameters could be estimated from a data frame. One way is defining the model in <code>gaussian-model.stan</code>.</p>
<p>Two remarks about the above.</p>
<ul>
<li>we use a <code>for</code> loop instead of vectorizing.</li>
<li>we model the predictor variable, <code>x</code>, as a random outcome of a Bernoulli distribution with some underlying parameter <code>x_mu</code>.</li>
</ul>
<p>Neither is strictly necessary, but this form will will become very obvious shortly. If we fit these models, we get a good agreement with the input parameters:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>   rm(list=ls())
</span></span><span style="display:flex;"><span>    d &lt;- read.csv(<span style="color:#a31515">&#34;./simulate_data/test_data.csv&#34;</span>, stringsAsFactors=<span style="color:#00f">FALSE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    N &lt;- nrow(d)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    m_data &lt;- list(
</span></span><span style="display:flex;"><span>        N=N,
</span></span><span style="display:flex;"><span>        y=d$y,
</span></span><span style="display:flex;"><span>        x=d$x
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real&lt;lower=0,upper=1&gt; x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        vector[N] mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x[i] ~ bernoulli(x_mu);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            mu[i] = a + b*x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }//i
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        y ~ normal(mu, 1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    &#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    library(rethinking)
</span></span><span style="display:flex;"><span>    m &lt;- stan( 
</span></span><span style="display:flex;"><span>        model_code=m_code , 
</span></span><span style="display:flex;"><span>        data=m_data,
</span></span><span style="display:flex;"><span>        chains=1,
</span></span><span style="display:flex;"><span>        refresh=0)
</span></span><span style="display:flex;"><span>    precis(m)
</span></span></code></pre></div><h2 id="incorporating-missingness">incorporating missingness</h2>
<p>Now that the model is properly defined, we turn to the topic of concern: what to do with missing values?</p>
<p>The shitty solution, which is unfortunately too common, is to just drop cases. But we can do better!</p>
<p>Above we calculate a likelihood of y given particular values of x, i.e. $f(y)=Pr(Y=y|x)$. But now we have a situation where we do not know x itself for some observations. What to do?</p>
<p>In the continous case, missing predictor values can be handled by sampling from the joint prior distribution within the model. This is not possible with a binary predictor, because &lt;reasons&gt;.</p>
<p>Instead we use <em>marginalization</em>, which allows us to describe the likelihood function of Y <em>even when we do not know particular values of X</em>, by the fact that:</p>
<p>$$
Pr(Y=y) = \sum^{\mathrm{all};i} Pr(Y=y | X=i) Pr(X=i)
$$</p>
<p>All we need then is a probability model for <code>x</code> that will provide the values, and then use them to average across the conditional probabilities for each observed value in <code>y</code>.</p>
<p>In this case, $Pr(X=1)$ is simply the mean of the population frequency <code>x_mu</code>, extra parameter we introduced to model <code>x</code> as a random outcome.</p>
<p>We can incoporate marginalization by</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>data{
</span></span><span style="display:flex;"><span>    int N;
</span></span><span style="display:flex;"><span>    int x[N];
</span></span><span style="display:flex;"><span>    int x_miss[N];
</span></span><span style="display:flex;"><span>    real y[N];
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>parameters{
</span></span><span style="display:flex;"><span>    real a;
</span></span><span style="display:flex;"><span>    real b;
</span></span><span style="display:flex;"><span>    real&lt;lower=0,upper=1&gt; x_mu;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>model{
</span></span><span style="display:flex;"><span>    a ~ normal(0,10);
</span></span><span style="display:flex;"><span>    b ~ normal(0,1);
</span></span><span style="display:flex;"><span>    x_mu ~ beta(1,1);
</span></span><span style="display:flex;"><span>    for ( i in 1:N ) {
</span></span><span style="display:flex;"><span>        if ( x_miss[i]==1 ) {
</span></span><span style="display:flex;"><span>            // x missing
</span></span><span style="display:flex;"><span>            target += log_mix( x_mu ,
</span></span><span style="display:flex;"><span>                    normal_lpdf( y[i] | a + b , 1 ),
</span></span><span style="display:flex;"><span>                    normal_lpdf( y[i] | a , 1 )
</span></span><span style="display:flex;"><span>                );
</span></span><span style="display:flex;"><span>        } else {
</span></span><span style="display:flex;"><span>            // x not missing
</span></span><span style="display:flex;"><span>            x[i] ~ bernoulli(x_mu);
</span></span><span style="display:flex;"><span>            y[i] ~ normal( a + b*x[i] , 1 );
</span></span><span style="display:flex;"><span>        } 
</span></span><span style="display:flex;"><span>    }//i
</span></span><span style="display:flex;"><span>}//model
</span></span></code></pre></div><p>This is only a small modification of the complete cases model.</p>
<p>We can also add another block in the STAN specification, to calculate the imputed predictor value for each missing case.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>generated quantities{
</span></span><span style="display:flex;"><span>    vector[N] x_impute;
</span></span><span style="display:flex;"><span>    for ( i in 1:N ) {
</span></span><span style="display:flex;"><span>        real logPxy;
</span></span><span style="display:flex;"><span>        real logPy;
</span></span><span style="display:flex;"><span>        if ( x_miss[i]==1 ) {
</span></span><span style="display:flex;"><span>            // need P(x|y)
</span></span><span style="display:flex;"><span>            // P(x|y) = P(x,y)/P(y)
</span></span><span style="display:flex;"><span>            // P(x,y) = P(x)P(y|x)
</span></span><span style="display:flex;"><span>            // P(y) = P(x==1)P(y|x==1) + P(x==0)P(y|x==0)
</span></span><span style="display:flex;"><span>            logPxy = log(x_mu) + normal_lpdf(y[i]|a+b,1);
</span></span><span style="display:flex;"><span>            logPy = log_mix( x_mu ,
</span></span><span style="display:flex;"><span>                    normal_lpdf( y[i] | a + b , 1 ),
</span></span><span style="display:flex;"><span>                    normal_lpdf( y[i] | a , 1 ) );
</span></span><span style="display:flex;"><span>            x_impute[i] = exp( logPxy - logPy );
</span></span><span style="display:flex;"><span>        } else {
</span></span><span style="display:flex;"><span>            x_impute[i] = x[i];
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }//i
</span></span><span style="display:flex;"><span>}//gq
</span></span></code></pre></div><p>With the above we try another simulation, this time creating holes in the dataset.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#008000"># validate marginalizing out missing binary predictor</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000 <span style="color:#008000"># number of cases</span>
</span></span><span style="display:flex;"><span>N_miss &lt;- 100 <span style="color:#008000"># number missing values</span>
</span></span><span style="display:flex;"><span>x_baserate &lt;- 0.5 <span style="color:#008000"># prob x==1 in total sample</span>
</span></span><span style="display:flex;"><span>a &lt;- 0 <span style="color:#008000"># intercept in y ~ N( a+b*x , 1 )</span>
</span></span><span style="display:flex;"><span>b &lt;- 1 <span style="color:#008000"># slope in y ~ N( a+b*x , 1 )</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># simulate data</span>
</span></span><span style="display:flex;"><span>x &lt;- sample( 0:1 , size=N , replace=<span style="color:#00f">TRUE</span> , prob=c(1-x_baserate,x_baserate) )
</span></span><span style="display:flex;"><span>i_miss &lt;- sample( 1:N , size=N_miss ) <span style="color:#008000"># model of missingness goes here</span>
</span></span><span style="display:flex;"><span>x_obs &lt;- x
</span></span><span style="display:flex;"><span>x_obs[i_miss] &lt;- (-1)
</span></span><span style="display:flex;"><span>x_miss &lt;- ifelse( 1:N %in% i_miss , 1 , 0 )
</span></span><span style="display:flex;"><span>y &lt;- rnorm( N , a + b*x , 1 )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_data &lt;- list(
</span></span><span style="display:flex;"><span>    N=N,
</span></span><span style="display:flex;"><span>    y=y,
</span></span><span style="display:flex;"><span>    x=x_obs,
</span></span><span style="display:flex;"><span>    x_miss=x_miss
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x_miss[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real&lt;lower=0,upper=1&gt; x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    x_mu ~ beta(1,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if ( x_miss[i]==1 ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // x missing
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            target += log_mix( x_mu ,
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                    normal_lpdf( y[i] | a + b , 1 ),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                    normal_lpdf( y[i] | a , 1 )
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // x not missing
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x[i] ~ bernoulli(x_mu);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            y[i] ~ normal( a + b*x[i] , 1 );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }//i
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//model
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] x_impute;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPxy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if ( x_miss[i]==1 ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // need P(x|y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x|y) = P(x,y)/P(y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x,y) = P(x)P(y|x)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(y) = P(x==1)P(y|x==1) + P(x==0)P(y|x==0)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPxy = log(x_mu) + normal_lpdf(y[i]|a+b,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPy = log_mix( x_mu ,
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                    normal_lpdf( y[i] | a + b , 1 ),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                    normal_lpdf( y[i] | a , 1 ) );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = exp( logPxy - logPy );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }//i
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//gq
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>m2 &lt;- stan( 
</span></span><span style="display:flex;"><span>    model_code=m_code , 
</span></span><span style="display:flex;"><span>    data=m_data,
</span></span><span style="display:flex;"><span>    chains=1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>precis(m2)
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m2)
</span></span><span style="display:flex;"><span>Px &lt;- apply(post$x_impute,2,median)
</span></span><span style="display:flex;"><span>pt1 &lt;- mean( Px[x_miss==1 &amp; x==1] )
</span></span><span style="display:flex;"><span>pt0 &lt;- mean( Px[x_miss==1 &amp; x==0] )
</span></span><span style="display:flex;"><span>plot( x[x_miss==1] , Px[x_miss==1] , ylim=c(0,1) , xlab=<span style="color:#a31515">&#34;true&#34;</span> , 
</span></span><span style="display:flex;"><span>    ylab=<span style="color:#a31515">&#34;imputed probability == 1&#34;</span> )
</span></span><span style="display:flex;"><span>points( c(0,1) , c(pt0,pt1) , pch=16 , col=<span style="color:#a31515">&#34;red&#34;</span> )
</span></span></code></pre></div><p>We&rsquo;ve successfully recovered the parameters we wanted to estimate <em>without throwing away any of the outcome observations</em>, which we would have had to do if we simply dropped cases.</p>
<h2 id="marginalization-with-a-binary-outcome-variable">marginalization with a binary outcome variable</h2>
<p>It is easy to swap out, code below:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># bernoulli without missingness</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># y ~ Bern( logistic(a+b*x) )</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># x is 0/1 predictor variable</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>x_baserate &lt;- 0.5
</span></span><span style="display:flex;"><span>a &lt;- 0
</span></span><span style="display:flex;"><span>b &lt;- 1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># simulate data</span>
</span></span><span style="display:flex;"><span>x &lt;- sample( 0:1 , size=N , replace=<span style="color:#00f">TRUE</span> , prob=c(1-x_baserate,x_baserate) )
</span></span><span style="display:flex;"><span>y &lt;- rbinom( N , 1, logistic(a + b*x) )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_data &lt;- list(
</span></span><span style="display:flex;"><span>    N=N,
</span></span><span style="display:flex;"><span>    y=y,
</span></span><span style="display:flex;"><span>    x=x
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real&lt;lower=0,upper=1&gt; x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] p;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for(i in 1:N){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        x[i] ~ bernoulli(x_mu)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        p[i] = a + b*x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    y ~ bernoulli_logit(p);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//model
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># and again you could include x_mu if you wanted to, since it is used in the marginalization</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>m &lt;- stan( 
</span></span><span style="display:flex;"><span>    model_code=m_code , 
</span></span><span style="display:flex;"><span>    data=m_data,
</span></span><span style="display:flex;"><span>    chains=1 )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>precis(m)
</span></span></code></pre></div><p>Ok now we do the same missingness model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>N_miss &lt;- 100
</span></span><span style="display:flex;"><span>x_baserate &lt;- 0.5
</span></span><span style="display:flex;"><span>a &lt;- 0
</span></span><span style="display:flex;"><span>b &lt;- 1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># simulate data</span>
</span></span><span style="display:flex;"><span>x &lt;- sample( 0:1 , size=N , replace=<span style="color:#00f">TRUE</span> , prob=c(1-x_baserate,x_baserate) )
</span></span><span style="display:flex;"><span>i_miss &lt;- sample( 1:N , size=N_miss ) <span style="color:#008000"># model of missingness goes here, </span>
</span></span><span style="display:flex;"><span>x_obs &lt;- x
</span></span><span style="display:flex;"><span>x_obs[i_miss] &lt;- (-1)
</span></span><span style="display:flex;"><span>x_miss &lt;- ifelse( 1:N %in% i_miss , 1 , 0 )
</span></span><span style="display:flex;"><span>y &lt;- rbinom( N , 1, logistic(a + b*x) )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_data &lt;- list(
</span></span><span style="display:flex;"><span>    N=N,
</span></span><span style="display:flex;"><span>    y=y,
</span></span><span style="display:flex;"><span>    x=x_obs,
</span></span><span style="display:flex;"><span>    x_miss=x_miss
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x_miss[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real&lt;lower=0,upper=1&gt; x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] p;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    x_mu ~ beta(1,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for(i in 1:N){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if(x_miss[i]==1){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            target += log_mix( x_mu, 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a + b)),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a))
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x[i] ~ bernoulli(x_mu);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            p[i] = a + b*x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            y[i] ~ bernoulli_logit(p[i]);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//model
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] x_impute;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPxy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if ( x_miss[i]==1 ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // need P(x|y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x|y) = P(x,y)/P(y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x,y) = P(x)P(y|x)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(y) = P(x==1)P(y|x==1) + P(x==0)P(y|x==0)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPxy = log(x_mu) + bernoulli_lpmf(y[i]|inv_logit(a+b));
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPy = log_mix( x_mu, 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a + b)),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a))
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = exp( logPxy - logPy );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }//i
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//gq
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m &lt;- stan( 
</span></span><span style="display:flex;"><span>    model_code=m_code , 
</span></span><span style="display:flex;"><span>    data=m_data,
</span></span><span style="display:flex;"><span>    chains=1 )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>precis(m)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract.samples(m)
</span></span><span style="display:flex;"><span>Px &lt;- apply(post$x_impute,2,median)
</span></span><span style="display:flex;"><span>pt1 &lt;- mean( Px[x_miss==1 &amp; x==1] )
</span></span><span style="display:flex;"><span>pt0 &lt;- mean( Px[x_miss==1 &amp; x==0] )
</span></span><span style="display:flex;"><span>plot( x[x_miss==1] , Px[x_miss==1] , ylim=c(0,1) , xlab=<span style="color:#a31515">&#34;true&#34;</span> , 
</span></span><span style="display:flex;"><span>    ylab=<span style="color:#a31515">&#34;imputed probability == 1&#34;</span> )
</span></span><span style="display:flex;"><span>points( c(0,1) , c(pt0,pt1) , pch=16 , col=<span style="color:#a31515">&#34;red&#34;</span> )
</span></span></code></pre></div><h2 id="adding-a-second-predictor">adding a second predictor</h2>
<p>The above code will not handle more than one predictor. Let&rsquo;s develop a more robust approach.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>x_baserate &lt;- 0.5
</span></span><span style="display:flex;"><span>a &lt;- 0
</span></span><span style="display:flex;"><span>b &lt;- 1
</span></span><span style="display:flex;"><span>b2 &lt;- -0.3
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x &lt;- sample( 0:1 , size=N , replace=<span style="color:#00f">TRUE</span> , prob=c(1-x_baserate,x_baserate) )
</span></span><span style="display:flex;"><span>x2 &lt;- rnorm( N, 10, 2 )
</span></span><span style="display:flex;"><span>y &lt;- rbinom( N , 1, logistic(a + b*x + b2*x2) )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_data &lt;- list(
</span></span><span style="display:flex;"><span>    N=N,
</span></span><span style="display:flex;"><span>    y=y,
</span></span><span style="display:flex;"><span>    x=x,
</span></span><span style="display:flex;"><span>    x2=x2
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real x2[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b2;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] p;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b2 ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for(i in 1:N){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        p[i] = a + b*x[i] + b2*x2[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    y ~ bernoulli_logit(p);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//model
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>m &lt;- stan( 
</span></span><span style="display:flex;"><span>    model_code=m_code , 
</span></span><span style="display:flex;"><span>    data=m_data,
</span></span><span style="display:flex;"><span>    chains=1 )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>precis(m)
</span></span></code></pre></div><h2 id="now-with-missingness">Now with missingness</h2>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N &lt;- 1000
</span></span><span style="display:flex;"><span>N_miss &lt;- 100
</span></span><span style="display:flex;"><span>x_baserate &lt;- 0.5
</span></span><span style="display:flex;"><span>a &lt;- 0
</span></span><span style="display:flex;"><span>b &lt;- 1
</span></span><span style="display:flex;"><span>b2 &lt;- -0.3
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># simulate data</span>
</span></span><span style="display:flex;"><span>x &lt;- sample( 0:1 , size=N , replace=<span style="color:#00f">TRUE</span> , prob=c(1-x_baserate,x_baserate) )
</span></span><span style="display:flex;"><span>i_miss &lt;- sample( 1:N , size=N_miss ) <span style="color:#008000"># model of missingness goes here, </span>
</span></span><span style="display:flex;"><span>x_obs &lt;- x
</span></span><span style="display:flex;"><span>x_obs[i_miss] &lt;- (-1)
</span></span><span style="display:flex;"><span>x_miss &lt;- ifelse( 1:N %in% i_miss , 1 , 0 )
</span></span><span style="display:flex;"><span>x2 &lt;- rnorm( N, 10, 2 )
</span></span><span style="display:flex;"><span>y &lt;- rbinom( N , 1, logistic(a + b*x + b2*x2) )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_data &lt;- list(
</span></span><span style="display:flex;"><span>    N=N,
</span></span><span style="display:flex;"><span>    y=y,
</span></span><span style="display:flex;"><span>    x=x_obs,
</span></span><span style="display:flex;"><span>    x_miss=x_miss,
</span></span><span style="display:flex;"><span>    x2=x2
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x_miss[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real x2[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b2;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real&lt;lower=0,upper=1&gt; x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] p;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    x_mu ~ beta(1,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b2 ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for(i in 1:N){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if(x_miss[i]==1){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            target += log_mix( x_mu, 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a + b + b2*x2[i])),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a + b2*x2[i]))
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x[i] ~ bernoulli(x_mu);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            p[i] = a + b*x[i] + b2*x2[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            y[i] ~ bernoulli_logit(p[i]);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//model
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] x_impute;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPxy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if ( x_miss[i]==1 ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // need P(x|y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x|y) = P(x,y)/P(y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x,y) = P(x)P(y|x)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(y) = P(x==1)P(y|x==1) + P(x==0)P(y|x==0)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPxy = log(x_mu) + bernoulli_lpmf(y[i]|inv_logit(a+b+b2*x2[i]));
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPy = log_mix( x_mu, 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a + b + b2*x2[i])),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(a + b2*x2[i]))
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = exp( logPxy - logPy );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }//i
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//gq
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># richard has an alternative to the log_mix that involves log_sum_exp, </span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># should be more generalizable</span>
</span></span><span style="display:flex;"><span>m_code &lt;- <span style="color:#a31515">&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">data{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int N;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int x_miss[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real x2[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    int y[N];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">parameters{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real a;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real b2;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real&lt;lower=0,upper=1&gt; x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">model{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] p;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[2] xprob;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real linear_operator;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    a ~ normal(0,10);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    x_mu ~ beta(1,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    b2 ~ normal(0,1);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    xprob[1] = 1 - x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    xprob[2] = x_mu;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for(i in 1:N){
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if(x_miss[i]==1){ //if any x2&#39;s were missing, need four logical calls here not 2
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            vector[2] terms;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            linear_operator = a + b2*x2[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            for ( xval in 0:1 ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                terms[xval+1] = log(xprob[xval+1]) + 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                          bernoulli_lpmf(y[i] | inv_logit(b*xval + linear_operator));
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            target += log_sum_exp( terms );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x[i] ~ bernoulli(x_mu);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            p[i] = a + b*x[i] + b2*x2[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            y[i] ~ bernoulli_logit(p[i]);
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//model
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">generated quantities{
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    vector[N] x_impute;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    real linear_operator;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    for ( i in 1:N ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPxy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        real logPy;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        if ( x_miss[i]==1 ) {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // need P(x|y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x|y) = P(x,y)/P(y)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(x,y) = P(x)P(y|x)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            // P(y) = P(x==1)P(y|x==1) + P(x==0)P(y|x==0)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            linear_operator = a + b2*x2[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPxy = log(x_mu) + bernoulli_lpmf(y[i]|inv_logit(linear_operator+b));
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            logPy = log_mix( x_mu, 
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(linear_operator + b)),
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">                bernoulli_lpmf(y[i] | inv_logit(linear_operator))
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = exp( logPxy - logPy );
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        } else {
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            x_impute[i] = x[i];
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        }
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    }//i
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">}//gq
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># it might be more conceptually useful to seperate out the x[i] ~ line in its own 1:N loop</span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># </span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>m &lt;- stan( 
</span></span><span style="display:flex;"><span>    model_code=m_code , 
</span></span><span style="display:flex;"><span>    data=m_data,
</span></span><span style="display:flex;"><span>    chains=1 )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>precis(m)
</span></span></code></pre></div><h1 id="15---syntax-wrappers">15 - syntax wrappers</h1>
<h2 id="rethinking-syntax">rethinking syntax</h2>
<p>Richard&rsquo;s developed a function, <code>map2stan</code>, which translates models very quickly. Here I go through some examples of the rethinking and equivalent STAN syntax, as well as rethinking&rsquo;s limitations.</p>
<pre tabindex="0"><code>
model0 &lt;- alist(
  y ~ dbinom(1, p),
  logit(p) &lt;- a,
  a ~ dnorm(0, 10)
)
</code></pre><p>does this not imply we can pass a and b and x as either data or parameters??</p>
<p>I&rsquo;ve written a function, <code>map2stan_model</code>, which returns the stan model for any map2stan alist.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>map2stan_model &lt;- function(flist, data, path = <span style="color:#00f">NA</span>, <span style="color:#00f">...</span>){
</span></span><span style="display:flex;"><span>  x &lt;- map2stan(flist, sample = <span style="color:#00f">FALSE</span>, data = data, <span style="color:#00f">...</span>)$model
</span></span><span style="display:flex;"><span>  x &lt;- gsub(<span style="color:#a31515">&#34;    &#34;</span>, <span style="color:#a31515">&#34;  &#34;</span>, x)
</span></span><span style="display:flex;"><span>  out &lt;- strsplit(x, <span style="color:#a31515">&#34;\n&#34;</span>)[[1]]
</span></span><span style="display:flex;"><span>  if(!is.na(path)){
</span></span><span style="display:flex;"><span>    out[1] &lt;- paste0(<span style="color:#a31515">&#34;// saved as &#34;</span>, path)
</span></span><span style="display:flex;"><span>    writeLines(out, path)
</span></span><span style="display:flex;"><span>    print(paste0(<span style="color:#a31515">&#34;model written to &#34;</span>, path))
</span></span><span style="display:flex;"><span>  } else {
</span></span><span style="display:flex;"><span>    out &lt;- out[-1]
</span></span><span style="display:flex;"><span>    return(out)
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>By default, rethinking returns a map2stan fit. But you can also ask map2stan to return a stanfit object using <code>rawstanfit = TRUE</code>.</p>
<p>and here&rsquo;s the equivalent in STAN:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d &lt;- data.frame(y = numeric())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(map2stan_model(model0, data = d, log_lik = <span style="color:#00f">TRUE</span>, DIC = <span style="color:#00f">TRUE</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- alist(
</span></span><span style="display:flex;"><span>  y ~ dbinom(1, p),
</span></span><span style="display:flex;"><span>  logit(p) &lt;- a + b*x,
</span></span><span style="display:flex;"><span>  a ~ dnorm(0,10),
</span></span><span style="display:flex;"><span>  b ~ dnorm(0,1)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>which becomes</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d &lt;- data.frame(y = integer(), x = numeric())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(map2stan_model(model0, data = d, log_lik = <span style="color:#00f">TRUE</span>, DIC = <span style="color:#00f">TRUE</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- alist(
</span></span><span style="display:flex;"><span>  y ~ dpois(lambda),
</span></span><span style="display:flex;"><span>  log(lambda) &lt;- a,
</span></span><span style="display:flex;"><span>  a ~ dnorm(0, 10)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d &lt;- data.frame(y = numeric())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(map2stan_model(model0, data = d, log_lik = <span style="color:#00f">TRUE</span>, DIC = <span style="color:#00f">TRUE</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- alist(
</span></span><span style="display:flex;"><span>  y ~ dexp(lambda),
</span></span><span style="display:flex;"><span>  log(lambda) &lt;- a,
</span></span><span style="display:flex;"><span>  a ~ dnorm(0, 10)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d &lt;- data.frame(y = numeric())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(map2stan_model(model0, data = d, log_lik = <span style="color:#00f">TRUE</span>, DIC = <span style="color:#00f">TRUE</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- alist(
</span></span><span style="display:flex;"><span>  y ~ dgeom(p),
</span></span><span style="display:flex;"><span>  logit(p) &lt;- alpha,
</span></span><span style="display:flex;"><span>  alpha ~ dnorm(0, 10)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d &lt;- data.frame(y = numeric())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(map2stan_model(model0, data = d, log_lik = <span style="color:#00f">TRUE</span>, DIC = <span style="color:#00f">TRUE</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Here&rsquo;s a standard linear model in <code>rethinking</code> syntax:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model0 &lt;- alist(
</span></span><span style="display:flex;"><span>  y ~ dnorm(mu, sigma),
</span></span><span style="display:flex;"><span>  mu &lt;- a + (b * x),
</span></span><span style="display:flex;"><span>  a ~ dnorm(0, 10),
</span></span><span style="display:flex;"><span>  b ~ dnorm(0, 1),
</span></span><span style="display:flex;"><span>  sigma ~ dexp(1)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>does this not imply we can pass a and b and x as either data or parameters??</p>
<p>and here&rsquo;s the equivalent in STAN:</p>
<pre tabindex="0"><code>
d &lt;- data.frame(y = numeric(), x = numeric())

cat(map2stan_model(model0, data = d, log_lik = TRUE, DIC = TRUE), sep = &#34;\n&#34;)
</code></pre><p>I have a function map2stan_model which dumps the STAN code. I&rsquo;ve used this to reverse-engineer stan models!</p>
<h3 id="imputation">imputation</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(rethinking)
</span></span><span style="display:flex;"><span>data(milk)
</span></span><span style="display:flex;"><span>d &lt;- milk
</span></span><span style="display:flex;"><span>d$neocortex.prop &lt;- d$neocortex.perc / 100
</span></span><span style="display:flex;"><span>d$logmass &lt;- log(d$mass)
</span></span></code></pre></div><p>Here&rsquo;s a model but we drop missing cases:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dcc &lt;- d[ complete.cases(d$neocortex.prop) , ]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data_list_cc &lt;- list(
</span></span><span style="display:flex;"><span>  kcal = dcc$kcal.per.g,
</span></span><span style="display:flex;"><span>  neocortex = dcc$neocortex.prop,
</span></span><span style="display:flex;"><span>  logmass = dcc$logmass
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model14.3cc &lt;- alist(
</span></span><span style="display:flex;"><span>  kcal ~ dnorm( mu , sigma ),
</span></span><span style="display:flex;"><span>  mu &lt;- a + bN * neocortex + bM * logmass,
</span></span><span style="display:flex;"><span>  a ~ dnorm( 0 , 100 ),
</span></span><span style="display:flex;"><span>  c(bN, bM) ~ dnorm(0 , 10),
</span></span><span style="display:flex;"><span>  sigma ~ dcauchy( 0 , 1 )
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit14.3cc &lt;- map2stan(model14.3cc, chains = 3, data = data_list_cc)
</span></span></code></pre></div><p>and the equivalent STAN model</p>
<pre tabindex="0"><code>
cat(map2stan_model(model14.3cc, data = data_list_cc, log_lik = TRUE, DIC = TRUE), sep = &#34;\n&#34;)
</code></pre><p>Now, rather than drop cases let&rsquo;s impute on neocortex.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data_list &lt;- list(
</span></span><span style="display:flex;"><span>  kcal = d$kcal.per.g,
</span></span><span style="display:flex;"><span>  neocortex = d$neocortex.prop,
</span></span><span style="display:flex;"><span>  logmass = d$logmass )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model14.3 &lt;- alist(
</span></span><span style="display:flex;"><span>  kcal ~ dnorm( mu , sigma ),
</span></span><span style="display:flex;"><span>  mu &lt;- a + bN * neocortex + bM * logmass,
</span></span><span style="display:flex;"><span>  neocortex ~ dnorm( nu, sigma_N ),
</span></span><span style="display:flex;"><span>  a ~ dnorm( 0 , 100 ),
</span></span><span style="display:flex;"><span>  c(bN, bM) ~ dnorm(0 , 10),
</span></span><span style="display:flex;"><span>  nu ~ dnorm( 0.5 , 1 ),
</span></span><span style="display:flex;"><span>  sigma_N ~ dcauchy( 0 , 1 ),
</span></span><span style="display:flex;"><span>  sigma ~ dcauchy( 0 , 1 )
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>and the STAN model</p>
<pre tabindex="0"><code>
cat(map2stan_model(m14.3, data = data_list, log_lik = TRUE, DIC = TRUE), sep = &#34;\n&#34;)
</code></pre><p>You need to create new variables to pass into such a model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>na_to_flag &lt;- function( x, y= -999 ) ifelse( is.na(x) , y , x )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>N_neocortex_missing &lt;- sum(is.na(d$neocortex.prop))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data_list &lt;- list(
</span></span><span style="display:flex;"><span>  N = nrow(d),
</span></span><span style="display:flex;"><span>  N_neocortex_missing = N_neocortex_missing,
</span></span><span style="display:flex;"><span>  neocortex_missingness = which(is.na(d$neocortex.prop)),
</span></span><span style="display:flex;"><span>  kcal = d$kcal.per.g,
</span></span><span style="display:flex;"><span>  neocortex = na_to_flag(d$neocortex.prop),
</span></span><span style="display:flex;"><span>  logmass = d$logmass 
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># m14.3 &lt;- map2stan(model14.3, data = data_list)</span>
</span></span></code></pre></div><pre tabindex="0"><code>
cat(map2stan_model(model14.3, data = data_list), sep = &#34;\n&#34;)
</code></pre><p>Finally, we make a more sophisticated model of missingness:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model14.4 &lt;- alist(
</span></span><span style="display:flex;"><span>  kcal ~ dnorm( mu , sigma ),
</span></span><span style="display:flex;"><span>  mu &lt;- a + bN * neocortex + bM * logmass,
</span></span><span style="display:flex;"><span>  neocortex ~ dnorm( nu, sigma_N ),
</span></span><span style="display:flex;"><span>  nu &lt;- a_N + gM * logmass,
</span></span><span style="display:flex;"><span>  a ~ dnorm( 0 , 100 ),
</span></span><span style="display:flex;"><span>  c(bN, bM, gM) ~ dnorm(0 , 10),
</span></span><span style="display:flex;"><span>  a_N ~ dnorm( 0.5, 1 ),
</span></span><span style="display:flex;"><span>  sigma_N ~ dcauchy( 0 , 1 ),
</span></span><span style="display:flex;"><span>  sigma ~ dcauchy( 0 , 1 )
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># m14.4 &lt;- map2stan(model14.4, data = data_list)</span>
</span></span></code></pre></div><pre tabindex="0"><code>
cat(map2stan_model(model14.4, data = data_list), sep = &#34;\n&#34;)
</code></pre><p>&hellip;</p>
<pre tabindex="0"><code>data{
  int&lt;lower=1&gt; N;
  int&lt;lower=1&gt; N_neocortex_missing;
  real kcal[N];
  real neocortex[N];
  int neocortex_missingness[N_neocortex_missing];
  real logmass[N];
}
parameters{
  vector[N_neocortex_missing] neocortex_impute;
  real a;
  real bN;
  real bM;
  real gM;
  real a_N;
  real&lt;lower=0&gt; sigma_N;
  real&lt;lower=0&gt; sigma;
}
transformed parameters{
  real neocortex_merge[N];
  neocortex_merge = neocortex;
  for ( u in 1:N_neocortex_missing ) neocortex_merge[neocortex_missingness[u]] = neocortex_impute[u];
}
model{
  vector[N] nu;
  vector[N] mu;
  sigma ~ cauchy( 0 , 1 );
  sigma_N ~ cauchy( 0 , 1 );
  a_N ~ normal( 0.5 , 1 );
  gM ~ normal( 0 , 10 );
  bM ~ normal( 0 , 10 );
  bN ~ normal( 0 , 10 );
  a ~ normal( 0 , 100 );
  for ( i in 1:N ) {
    nu[i] = a_N + gM * logmass[i];
  }
  neocortex_merge ~ normal( nu , sigma_N );
  for ( i in 1:N ) {
    mu[i] = a + bN * neocortex_merge[i] + bM * logmass[i];
  }
  kcal ~ normal( mu , sigma );
}
generated quantities{
  vector[N] nu;
  vector[N] mu;
  real dev;
  dev = 0;
  for ( i in 1:N ) {
    nu[i] = a_N + gM * logmass[i];
  }
  for ( i in 1:N ) {
    mu[i] = a + bN * neocortex_merge[i] + bM * logmass[i];
  }
  dev = dev + (-2)*normal_lpdf( kcal | mu , sigma );
}
</code></pre><h3 id="some-real-examples">some real examples</h3>
<p>Here&rsquo;s one of the models I used for a PPR analysis of the Mosuo</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model3 &lt;- alist(
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  conc ~ dbinom(1, p),
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  logit(p) &lt;- a[id] + 
</span></span><span style="display:flex;"><span>    b1*pcode10 + 
</span></span><span style="display:flex;"><span>    b2*pcode11 + 
</span></span><span style="display:flex;"><span>    b3*patrilineal + 
</span></span><span style="display:flex;"><span>    b4*pat_10 + 
</span></span><span style="display:flex;"><span>    b5*pat_11 + 
</span></span><span style="display:flex;"><span>    b6*age15 + 
</span></span><span style="display:flex;"><span>    b7*age20 + 
</span></span><span style="display:flex;"><span>    b8*yearbirth1930 + 
</span></span><span style="display:flex;"><span>    b9*yearbirth1940 + 
</span></span><span style="display:flex;"><span>    b10*yearbirth1950 + 
</span></span><span style="display:flex;"><span>    b11*yearbirth1970 + 
</span></span><span style="display:flex;"><span>    b12*yearbirth1920 + 
</span></span><span style="display:flex;"><span>    b14*mi_job + 
</span></span><span style="display:flex;"><span>    b15*highestgrade + 
</span></span><span style="display:flex;"><span>    b16*bly + 
</span></span><span style="display:flex;"><span>    b17*age30 + 
</span></span><span style="display:flex;"><span>    b18*age35 + 
</span></span><span style="display:flex;"><span>    b19*age40,
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  a[id] ~ dnorm(a_mu, a_sigma),
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  c(a_mu, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b14, b15, b16, b17, b18, b19) ~ dnorm(0, 100),
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  a_sigma ~ dcauchy(0,1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d &lt;- data.frame(
</span></span><span style="display:flex;"><span>  id = integer(),
</span></span><span style="display:flex;"><span>  conc = numeric(),
</span></span><span style="display:flex;"><span>  pcode10 = numeric(), 
</span></span><span style="display:flex;"><span>  pcode11 = numeric(), 
</span></span><span style="display:flex;"><span>  patrilineal = numeric(), 
</span></span><span style="display:flex;"><span>  pat_10 = numeric(), 
</span></span><span style="display:flex;"><span>  pat_11 = numeric(), 
</span></span><span style="display:flex;"><span>  age15 = numeric(), 
</span></span><span style="display:flex;"><span>  age20  = numeric(), 
</span></span><span style="display:flex;"><span>  yearbirth1930 = numeric(), 
</span></span><span style="display:flex;"><span>  yearbirth1940 = numeric(), 
</span></span><span style="display:flex;"><span>  yearbirth1950 = numeric(), 
</span></span><span style="display:flex;"><span>  yearbirth1970 = numeric(), 
</span></span><span style="display:flex;"><span>  yearbirth1920 = numeric(), 
</span></span><span style="display:flex;"><span>  mi_job = numeric(), 
</span></span><span style="display:flex;"><span>  highestgrade = numeric(), 
</span></span><span style="display:flex;"><span>  bly = numeric(), 
</span></span><span style="display:flex;"><span>  age30 = numeric(), 
</span></span><span style="display:flex;"><span>  age35 = numeric(), 
</span></span><span style="display:flex;"><span>  age40 = numeric()
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cat(map2stan_model(model3, data = d, log_lik = <span style="color:#00f">TRUE</span>, DIC = <span style="color:#00f">TRUE</span>), sep = <span style="color:#a31515">&#34;\n&#34;</span>)
</span></span></code></pre></div><p>Here&rsquo;s the Go model in <code>glmer2stan</code> syntax. I didn&rsquo;t write this in <code>rethinking</code> syntax, but I now understand that the model was doing some interesting things with the covariance of varying effects in ways I would not have anticipated if I had done it in <code>map2stan</code>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>horizon24 &lt;- glmer2stan(
</span></span><span style="display:flex;"><span>   fourfour ~ (1 + b.44 + pop.44 | PB.id) + (0 + b.44 + pop.44 | b.age.group) +
</span></span><span style="display:flex;"><span>  b.44 +
</span></span><span style="display:flex;"><span>  b.44xb.win.44 +
</span></span><span style="display:flex;"><span>  b.44xb.win +
</span></span><span style="display:flex;"><span>  b.win.44 +
</span></span><span style="display:flex;"><span>  pop.44 +
</span></span><span style="display:flex;"><span>  pop.44xpop.win.44 +
</span></span><span style="display:flex;"><span>  pop.44xb.win +
</span></span><span style="display:flex;"><span>  pop.win.44 +
</span></span><span style="display:flex;"><span>  komi,
</span></span><span style="display:flex;"><span>   data=d,
</span></span><span style="display:flex;"><span>   family=<span style="color:#a31515">&#34;binomial&#34;</span>,
</span></span><span style="display:flex;"><span>   warmup=100,
</span></span><span style="display:flex;"><span>   iter=200,
</span></span><span style="display:flex;"><span>   calcDIC=<span style="color:#00f">TRUE</span>,  
</span></span><span style="display:flex;"><span>   initmethod=<span style="color:#a31515">&#34;zero&#34;</span>
</span></span><span style="display:flex;"><span>) 
</span></span></code></pre></div><p>and in STAN</p>
<pre tabindex="0"><code>
data{
  int N;
  int fourfour[N];
  real b_44[N];
  real pop_44[N];
  int PB_id[N];
  int b_age_group[N];
  real b_44xb_win_44[N];
  real b_44xb_win[N];
  real b_win_44[N];
  real pop_44xpop_win_44[N];
  real pop_44xb_win[N];
  real pop_win_44[N];
  real komi[N];
  int bin_total[N];
  int N_PB_id;
  int N_b_age_group;
}

transformed data{
  vector[3] zeros_PB_id;
  vector[2] zeros_b_age_group;
  for ( i in 1:3 ) zeros_PB_id[i] &lt;- 0;
  for ( i in 1:2 ) zeros_b_age_group[i] &lt;- 0;
}

parameters{
  real Intercept;
  real beta_b_44;
  real beta_b_44xb_win_44;
  real beta_b_44xb_win;
  real beta_b_win_44;
  real beta_pop_44;
  real beta_pop_44xpop_win_44;
  real beta_pop_44xb_win;
  real beta_pop_win_44;
  real beta_komi;
  vector[3] vary_PB_id[N_PB_id];
  cov_matrix[3] Sigma_PB_id;
  vector[2] vary_b_age_group[N_b_age_group];
  cov_matrix[2] Sigma_b_age_group;
}

model{
  real vary[N];
  real glm[N];
  // Priors
  Intercept ~ normal( 0 , 100 );
  beta_b_44 ~ normal( 0 , 100 );
  beta_b_44xb_win_44 ~ normal( 0 , 100 );
  beta_b_44xb_win ~ normal( 0 , 100 );
  beta_b_win_44 ~ normal( 0 , 100 );
  beta_pop_44 ~ normal( 0 , 100 );
  beta_pop_44xpop_win_44 ~ normal( 0 , 100 );
  beta_pop_44xb_win ~ normal( 0 , 100 );
  beta_pop_win_44 ~ normal( 0 , 100 );
  beta_komi ~ normal( 0 , 100 );
  // Varying effects
  for ( j in 1:N_PB_id ) vary_PB_id[j] ~ multi_normal( zeros_PB_id , Sigma_PB_id );
  for ( j in 1:N_b_age_group ) vary_b_age_group[j] ~ multi_normal( zeros_b_age_group , Sigma_b_age_group );
  // Fixed effects
  for ( i in 1:N ) {
    vary[i] &lt;- vary_PB_id[PB_id[i],1]
        + vary_PB_id[PB_id[i],2] * b_44[i]
        + vary_PB_id[PB_id[i],3] * pop_44[i]
        + vary_b_age_group[b_age_group[i],1]
        + vary_b_age_group[b_age_group[i],2] * pop_44[i];
    glm[i] &lt;- vary[i] + Intercept
        + beta_b_44 * b_44[i]
        + beta_b_44xb_win_44 * b_44xb_win_44[i]
        + beta_b_44xb_win * b_44xb_win[i]
        + beta_b_win_44 * b_win_44[i]
        + beta_pop_44 * pop_44[i]
        + beta_pop_44xpop_win_44 * pop_44xpop_win_44[i]
        + beta_pop_44xb_win * pop_44xb_win[i]
        + beta_pop_win_44 * pop_win_44[i]
        + beta_komi * komi[i];
    glm[i] &lt;- inv_logit( glm[i] );
  }
  fourfour ~ binomial( bin_total , glm );
}

generated quantities{
  real dev;
  real vary[N];
  real glm[N];
  dev &lt;- 0;
  for ( i in 1:N ) {
    vary[i] &lt;- vary_PB_id[PB_id[i],1]
        + vary_PB_id[PB_id[i],2] * b_44[i]
        + vary_PB_id[PB_id[i],3] * pop_44[i]
        + vary_b_age_group[b_age_group[i],1]
        + vary_b_age_group[b_age_group[i],2] * pop_44[i];
    glm[i] &lt;- vary[i] + Intercept
        + beta_b_44 * b_44[i]
        + beta_b_44xb_win_44 * b_44xb_win_44[i]
        + beta_b_44xb_win * b_44xb_win[i]
        + beta_b_win_44 * b_win_44[i]
        + beta_pop_44 * pop_44[i]
        + beta_pop_44xpop_win_44 * pop_44xpop_win_44[i]
        + beta_pop_44xb_win * pop_44xb_win[i]
        + beta_pop_win_44 * pop_win_44[i]
        + beta_komi * komi[i];
    dev &lt;- dev + (-2) * binomial_log( fourfour[i] , bin_total[i] , inv_logit(glm[i]) );
  }
}
</code></pre><h2 id="brms-syntax">brms syntax</h2>
<p>Here is a survival model.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(<span style="color:#a31515">&#34;brms&#34;</span>)
</span></span><span style="display:flex;"><span>data(<span style="color:#a31515">&#34;kidney&#34;</span>, package = <span style="color:#a31515">&#34;brms&#34;</span>)
</span></span><span style="display:flex;"><span>head(kidney, n = 3)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit1 &lt;- brm(
</span></span><span style="display:flex;"><span>  formula =
</span></span><span style="display:flex;"><span>    time | cens(censored) ~ age * sex + disease + (1 + age|patient),
</span></span><span style="display:flex;"><span>  data = kidney, family = lognormal(),
</span></span><span style="display:flex;"><span>  prior = c(set_prior(<span style="color:#a31515">&#34;normal(0,5)&#34;</span>, class = <span style="color:#a31515">&#34;b&#34;</span>),
</span></span><span style="display:flex;"><span>   set_prior(<span style="color:#a31515">&#34;cauchy(0,2)&#34;</span>, class = <span style="color:#a31515">&#34;sd&#34;</span>),
</span></span><span style="display:flex;"><span>   set_prior(<span style="color:#a31515">&#34;lkj(2)&#34;</span>, class = <span style="color:#a31515">&#34;cor&#34;</span>)),
</span></span><span style="display:flex;"><span>  warmup = 1000, iter = 2000, chains = 4,
</span></span><span style="display:flex;"><span>  control = list(adapt_delta = 0.95))
</span></span></code></pre></div><p>The right side of the formula uses <code>lme4</code> syntax exactly.</p>
<ul>
<li>
<p>Linear and robust linear regression can be performed using the <code>gaussian</code> or <code>student</code> family combined with the <code>identity</code> link.</p>
</li>
<li>
<p>For dichotomous and categorical data, families <code>bernoulli</code>, <code>binomial</code>, and <code>categorical</code> combined with the <code>logit</code> link, by default, are perfectly suited.</p>
</li>
<li>
<p>Families <code>poisson</code>, <code>negbinomial</code>, and <code>geometric</code> allow for modeling count data.</p>
</li>
<li>
<p>Families <code>lognormal</code>, <code>Gamma</code>, <code>exponential</code>, and <code>weibull</code> can be used (among others) for survival regression.</p>
</li>
<li>
<p>Ordinal regression can be performed using the families <code>cumulative</code>, <code>cratio</code>, <code>sratio</code>, and <code>acat</code>.</p>
</li>
<li>
<p>Finally, excess zeros in the response can be adequately modeled using families <code>zero_inflated_poisson</code>, <code>zero_inflated_negbinomial</code>, <code>zero_inflated_binomial</code>, <code>zero_inflated_beta</code>, <code>hurdle_poisson</code>, <code>hurdle_negbinomial</code>, and <code>hurdle_gamma</code>.</p>
</li>
</ul>
<p>leave-one-out cross-validation (LOO)</p>
<p>The output is class <code>brmsfit</code></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> fit3 &lt;- brm(formula = rating ~ treat + period + carry + (1 | subject),
</span></span><span style="display:flex;"><span> data = inhaler, family = cumulative)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit4 &lt;- brm(
</span></span><span style="display:flex;"><span>  formula =
</span></span><span style="display:flex;"><span>    rating ~ period + carry + cse(treat) + (1 | subject),
</span></span><span style="display:flex;"><span>  data = inhaler, family = sratio(threshold = <span style="color:#a31515">&#34;equidistant&#34;</span>),
</span></span><span style="display:flex;"><span>  prior = set_prior(<span style="color:#a31515">&#34;normal(-1,2)&#34;</span>, coef = <span style="color:#a31515">&#34;treat&#34;</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit1 &lt;- brm(count ~ log_Age_c + log_Base4_c * Trt  
</span></span><span style="display:flex;"><span>                   + (1|patient) + (1|obs), 
</span></span><span style="display:flex;"><span>                 data = epilepsy, family = poisson(), 
</span></span><span style="display:flex;"><span>                 prior = c(prior(student_t(5,0,10), class = b),
</span></span><span style="display:flex;"><span>                           prior(cauchy(0,2), class = sd)))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span> fit2 &lt;- brm(rating ~ period + carry + cs(treat), 
</span></span><span style="display:flex;"><span>                 data = inhaler, family = sratio(<span style="color:#a31515">&#34;cloglog&#34;</span>), 
</span></span><span style="display:flex;"><span>                 prior = set_prior(<span style="color:#a31515">&#34;normal(0,5)&#34;</span>), chains = 2)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>  fit3 &lt;- brm(time | cens(censored) ~ age * sex + disease + (1|patient), 
</span></span><span style="display:flex;"><span>                 data = kidney, family = lognormal())
</span></span></code></pre></div><h3 id="probit-regression-using-the-binomial-family">probit regression using the binomial family</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>n &lt;- sample(1:10, 100, <span style="color:#00f">TRUE</span>)  <span style="color:#008000"># number of trials</span>
</span></span><span style="display:flex;"><span>success &lt;- rbinom(100, size = n, prob = 0.4)
</span></span><span style="display:flex;"><span>x &lt;- rnorm(100)
</span></span><span style="display:flex;"><span>data4 &lt;- data.frame(n, success, x)
</span></span><span style="display:flex;"><span>fit4 &lt;- brm(formula = success | trials(n) ~ x, data = data4,
</span></span><span style="display:flex;"><span>  family = binomial(<span style="color:#a31515">&#34;probit&#34;</span>))
</span></span></code></pre></div><p>Here&rsquo;s a function I wrote to extract the posterior samples for each paramter:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>extract_brms &lt;- function(brmsfit) brmsfit$fit@sim$samples[[1]]
</span></span></code></pre></div><h3 id="simple-linear-gaussian-model">simple linear gaussian model</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>x &lt;- rnorm(100)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(100, mean = 2 - 1.5 * x, sd = 1)
</span></span><span style="display:flex;"><span>data5 &lt;- data.frame(x, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit1 &lt;- brm(y ~ 1, data = data5)
</span></span><span style="display:flex;"><span>fit2 &lt;- brm(y ~ x, data = data5)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>post &lt;- extract_brms(fit2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit5 &lt;- brm(y ~ a1 + a2 * x,
</span></span><span style="display:flex;"><span>           data = data5,
</span></span><span style="display:flex;"><span>           prior = c(prior(normal(0, 2), nlpar = a1),
</span></span><span style="display:flex;"><span>                     prior(normal(0, 2), nlpar = a2)))
</span></span></code></pre></div><h3 id="simple-non-linear-gaussian-model">simple non-linear gaussian model</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>x &lt;- rnorm(100)
</span></span><span style="display:flex;"><span>y &lt;- rnorm(100, mean = 2 - 1.5^x, sd = 1)
</span></span><span style="display:flex;"><span>data5 &lt;- data.frame(x, y)
</span></span><span style="display:flex;"><span>fit5 &lt;- brm(bf(y ~ a1 - a2^x, a1 + a2 ~ 1, nl = <span style="color:#00f">TRUE</span>),  
</span></span><span style="display:flex;"><span>           data = data5,
</span></span><span style="display:flex;"><span>           prior = c(prior(normal(0, 2), nlpar = a1),
</span></span><span style="display:flex;"><span>                     prior(normal(0, 2), nlpar = a2)))
</span></span><span style="display:flex;"><span>summary(fit5)
</span></span><span style="display:flex;"><span>marginal_effects(fit5)
</span></span><span style="display:flex;"><span>plot(marginal_effects(fit5), ask = <span style="color:#00f">FALSE</span>)
</span></span></code></pre></div><h3 id="normal-model-with-heterogeneous-variances">normal model with heterogeneous variances</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>data_het &lt;- data.frame(y = c(rnorm(50), rnorm(50, 1, 2)),
</span></span><span style="display:flex;"><span>                      x = factor(rep(c(<span style="color:#a31515">&#34;a&#34;</span>, <span style="color:#a31515">&#34;b&#34;</span>), each = 50)))
</span></span><span style="display:flex;"><span>fit6 &lt;- brm(bf(y ~ x, sigma ~ 0 + x), data = data_het)
</span></span><span style="display:flex;"><span>summary(fit6)
</span></span><span style="display:flex;"><span>plot(fit6)
</span></span><span style="display:flex;"><span>marginal_effects(fit6)
</span></span><span style="display:flex;"><span><span style="color:#008000"># extract estimated residual SDs of both groups</span>
</span></span><span style="display:flex;"><span>sigmas &lt;- exp(posterior_samples(fit6, <span style="color:#a31515">&#34;^b_sigma_&#34;</span>))
</span></span><span style="display:flex;"><span>ggplot(stack(sigmas), aes(values)) + 
</span></span><span style="display:flex;"><span> geom_density(aes(fill = ind))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit7 &lt;- brm(bf(y ~ x, quantile = 0.25), data = data_het, 
</span></span><span style="display:flex;"><span>               family = asym_laplace())
</span></span><span style="display:flex;"><span>summary(fit7)
</span></span><span style="display:flex;"><span>marginal_effects(fit7)
</span></span></code></pre></div><p>A ZINB:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit_zinb1 &lt;- brm(
</span></span><span style="display:flex;"><span>  formula = count ~ persons + child + camper,
</span></span><span style="display:flex;"><span>  data = zinb,
</span></span><span style="display:flex;"><span>  family = zero_inflated_poisson(<span style="color:#a31515">&#34;log&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit_zinb2 &lt;- brm(
</span></span><span style="display:flex;"><span>  formula = bf(count ~ persons + child + camper, zi ~ child),
</span></span><span style="display:flex;"><span>  data = zinb,
</span></span><span style="display:flex;"><span>  family = zero_inflated_poisson()
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The structure of a brmsfit object call m is:</p>
<p><code>m$fit</code> is a <code>stanfit</code> object, so you can explore that as any stan model. Posteriors can be extracted using</p>
<p><code>m$fit@sim$samples</code></p>
<p>and</p>
<p><code>m$fit@stanmodel@model_code</code></p>
<p>give you the model code created by brms.</p>
<p>see rstanarm (Gabry and Goodrich 2017)</p>
<p>Here&rsquo;s a zi neg binom model</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>library(brms)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n &lt;- 10000
</span></span><span style="display:flex;"><span>x1 &lt;- rnorm(n, 0, 1)
</span></span><span style="display:flex;"><span>x2 &lt;- rnorm(n, 0, 1)
</span></span><span style="display:flex;"><span>y &lt;- rnbinom(n, mu = exp(0.3 + 0.2 * x1 - 0.4 * x2), 1)
</span></span><span style="display:flex;"><span>die &lt;- rbinom(n, size = 1, prob = logistic(-3 + 2 * x2))
</span></span><span style="display:flex;"><span>y[which(die == 1)] &lt;- 0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dm &lt;- data.frame(x1, x2, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model1 &lt;- bf(
</span></span><span style="display:flex;"><span>  y ~ x1 + x2, 
</span></span><span style="display:flex;"><span>  zi ~ x2
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fit_zinb1 &lt;- brm(formula = model1, data = dm, family = zero_inflated_negbinomial())
</span></span></code></pre></div><h2 id="rstanarm-syntax-empty">rstanarm syntax (empty)</h2>
<p>(soon)</p>
<h1 id="16---parallelization">16 - parallelization</h1>
<p>cmdstan has the ability to compile programs that take extra arguments to allow multithreading</p>
<p>explanation here: <a href="https://mc-stan.org/docs/2_25/cmdstan-guide/parallelization.html">https://mc-stan.org/docs/2_25/cmdstan-guide/parallelization.html</a></p>
<p>however, the models themslevs have to be rewritten in a way that takes advantage, as decrbed in the &ldquo;Parallelization&rdquo; chapter now in the Stan maual (v2.27)</p>
<p>this is accomplished via two mechanisms &ldquo;reduce with summation&rdquo; and &ldquo;rectangual map&rdquo;</p>
<p>stan v2.26 and newer have a &lsquo;profiling&rsquo; feature that describe wihch parts of the pogram are computationalyy the most intensive
only portions of the Stan program run in parallel</p>
<p>looks like we can succesfully implement our model to run in parallel, but it requires some rewrites and experimentation wihch will take a few hours, and i can&rsquo;t really test it out except on the cluster, and i dont wanna right now</p>
<p>basic logistic regression:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">data {
  int N;
  array[N] int y;
  vector[N] x;
}
parameters {
  vector[2] beta;
}
model {
  beta ~ std_normal();
  for(n in 1:N) {
    target += bernoulli_logit_lpmf(y[n] | beta[1] + beta[2] * x[n])
  }
}
</code></pre><p>parallelized version:</p>
<pre tabindex="0"><code class="language-stan" data-lang="stan">functions {
  real partial_sum(array[] int y_slice, int start, int end, vector x, vector beta) {
    return bernoulli_logit_lpmf(y_slice | beta[1] + beta[2] * x[start:end]);
  }
}
data {
  int N;
  array[N] int y;
  vector[N] x;
}
parameters {
  vector[2] beta;
}
model {
  int grainsize = 1;
  beta ~ std_normal();
  target += reduce_sum(partial_sum, y, grainsize, x, beta);
}
</code></pre><p>work will be split until all partial sums are smaller than the grainsize</p>
<hr>

</content>
<p>
  
</p>

  </main>
  <footer>powered by <a href="https://gohugo.io">Hugo</a> with the <a href="https://github.com/janraasch/hugo-bearblog/">hugo-bearblog</a> theme
</footer>

    
</body>

</html>
